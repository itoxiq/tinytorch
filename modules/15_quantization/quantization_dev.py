# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.18.1
#   kernelspec:
#     display_name: Python 3 (ipykernel)
#     language: python
#     name: python3
# ---

# %%
#| default_exp optimization.quantization

# %% [markdown]
"""
# Module 17: Quantization - Making Models Smaller and Faster

Welcome to Quantization! Today you'll learn how to reduce model precision from FP32 to INT8 while preserving accuracy.

## ğŸ”— Prerequisites & Progress
**You've Built**: Complete ML pipeline with profiling and acceleration techniques
**You'll Build**: INT8 quantization system with calibration and memory savings
**You'll Enable**: 4Ã— memory reduction and 2-4Ã— speedup with minimal accuracy loss

**Connection Map**:
```
Profiling â†’ Quantization â†’ Compression
(measure)   (reduce bits)  (remove weights)
```

## Learning Objectives
By the end of this module, you will:
1. Implement INT8 quantization with proper scaling
2. Build quantization-aware training for minimal accuracy loss
3. Apply post-training quantization to existing models
4. Measure actual memory and compute savings
5. Understand quantization error and mitigation strategies

Let's make models 4Ã— smaller!
"""

# %% [markdown]
"""
## ğŸ“¦ Where This Code Lives in the Final Package

**Learning Side:** You work in `modules/17_quantization/quantization_dev.py`  
**Building Side:** Code exports to `tinytorch.optimization.quantization`

```python
# How to use this module:
from tinytorch.optimization.quantization import quantize_int8, QuantizedLinear, quantize_model
```

**Why this matters:**
- **Learning:** Complete quantization system in one focused module for deep understanding
- **Production:** Proper organization like PyTorch's torch.quantization with all optimization components together
- **Consistency:** All quantization operations and calibration tools in optimization.quantization
- **Integration:** Works seamlessly with existing models for complete optimization pipeline
"""

# %% nbgrader={"grade": false, "grade_id": "imports", "solution": true}
#| export
import numpy as np
import time
from typing import Tuple, Dict, List, Optional
import warnings

# Import dependencies from other modules
from tinytorch.core.tensor import Tensor
from tinytorch.core.layers import Linear
from tinytorch.core.activations import ReLU

print("âœ… Quantization module imports complete")

# %% [markdown]
"""
## 1. Introduction - The Memory Wall Problem

Imagine trying to fit a library in your backpack. Neural networks face the same challenge - models are getting huge, but devices have limited memory!

### The Precision Paradox

Modern neural networks use 32-bit floating point numbers with incredible precision:

```
FP32 Number: 3.14159265359...
             ^^^^^^^^^^^^^^^^
             32 bits = 4 bytes per weight
```

But here's the surprising truth: **we don't need all that precision for most AI tasks!**

### The Growing Memory Crisis

```
Model Memory Requirements (FP32):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BERT-Base:   110M params Ã—  4 bytes = 440MB                â”‚
â”‚ GPT-2:       1.5B params Ã—  4 bytes = 6GB                  â”‚
â”‚ GPT-3:       175B params Ã— 4 bytes = 700GB                 â”‚
â”‚ Your Phone:  Available RAM = 4-8GB                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†‘
                    Problem!
```

### The Quantization Solution

What if we could represent each weight with just 8 bits instead of 32?

```
Before Quantization (FP32):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3.14159265  â”‚  2.71828183  â”‚   â”‚  32 bits each
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

After Quantization (INT8):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   98   â”‚   85   â”‚   72   â”‚   45   â”‚  8 bits each
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘
    4Ã— less memory!
```

### Real-World Impact You'll Achieve

**Memory Reduction:**
- BERT-Base: 440MB â†’ 110MB (4Ã— smaller)
- Fits on mobile devices!
- Faster loading from disk
- More models in GPU memory

**Speed Improvements:**
- 2-4Ã— faster inference (hardware dependent)
- Lower power consumption
- Better user experience

**Accuracy Preservation:**
- <1% accuracy loss with proper techniques
- Sometimes even improves generalization!

**Why This Matters:**
- **Mobile AI:** Deploy powerful models on phones
- **Edge Computing:** Run AI without cloud connectivity
- **Data Centers:** Serve more users with same hardware
- **Environmental:** Reduce energy consumption by 2-4Ã—

Today you'll build the production-quality quantization system that makes all this possible!
"""

# %% [markdown]
"""
## 2. Foundations - The Mathematics of Compression

### Understanding the Core Challenge

Think of quantization like converting a smooth analog signal to digital steps. We need to map infinite precision (FP32) to just 256 possible values (INT8).

### The Quantization Mapping

```
The Fundamental Problem:

FP32 Numbers (Continuous):        INT8 Numbers (Discrete):
    âˆ possible values         â†’      256 possible values

  ...  -1.7  -1.2  -0.3  0.0  0.8  1.5  2.1  ...
         â†“     â†“     â†“    â†“    â†“    â†“    â†“
      -128  -95   -38    0   25   48   67   127
```

### The Magic Formula

Every quantization system uses this fundamental relationship:

```
Quantization (FP32 â†’ INT8):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  quantized = round((float_value - zero_point) / scale)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Dequantization (INT8 â†’ FP32):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  float_value = scale Ã— quantized + zero_point          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Two Critical Parameters

**1. Scale (s)** - How big each INT8 step is in FP32 space:
```
Small Scale (high precision):       Large Scale (low precision):
 FP32: [0.0, 0.255]                 FP32: [0.0, 25.5]
   â†“     â†“     â†“                       â†“     â†“     â†“
 INT8:  0    128   255              INT8:  0    128   255
        â”‚     â”‚     â”‚                      â”‚     â”‚     â”‚
      0.0   0.127  0.255                 0.0   12.75  25.5

 Scale = 0.001 (very precise)        Scale = 0.1 (less precise)
```

**2. Zero Point (z)** - Which INT8 value represents FP32 zero:
```
Symmetric Range:                    Asymmetric Range:
 FP32: [-2.0, 2.0]                  FP32: [-1.0, 3.0]
   â†“     â†“     â†“                       â†“     â†“     â†“
 INT8: -128    0   127              INT8: -128   64   127
        â”‚     â”‚     â”‚                      â”‚     â”‚     â”‚
     -2.0    0.0   2.0                  -1.0   0.0   3.0

 Zero Point = 0                     Zero Point = 64
```

### Visual Example: Weight Quantization

```
Original FP32 Weights:           Quantized INT8 Mapping:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ -0.8  -0.3   0.0   0.5  â”‚  â†’   â”‚ -102  -38    0   64     â”‚
â”‚  0.9   1.2  -0.1   0.7  â”‚      â”‚  115  153  -13   89     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     4 bytes each                      1 byte each
   Total: 32 bytes                   Total: 8 bytes
                                    â†‘
                              4Ã— compression!
```

### Quantization Error Analysis

```
Perfect Reconstruction (Impossible):  Quantized Reconstruction (Reality):

Original: 0.73                       Original: 0.73
    â†“                                     â†“
INT8: ? (can't represent exactly)     INT8: 93 (closest)
    â†“                                     â†“
Restored: 0.73                        Restored: 0.728
                                           â†‘
                                    Error: 0.002
```

**The Quantization Trade-off:**
- **More bits** = Higher precision, larger memory
- **Fewer bits** = Lower precision, smaller memory
- **Goal:** Find the sweet spot where error is acceptable

### Why INT8 is the Sweet Spot

```
Precision vs Memory Trade-offs:

FP32: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (32 bits) - Overkill precision
FP16: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (16 bits)                  - Good precision
INT8: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (8 bits)                           - Sufficient precision â† Sweet spot!
INT4: â–ˆâ–ˆâ–ˆâ–ˆ (4 bits)                               - Often too little

Memory:    100%    50%    25%    12.5%
Accuracy:  100%   99.9%  99.5%   95%
```

INT8 gives us 4Ã— memory reduction with <1% accuracy loss - the perfect balance for production systems!
"""

# %% [markdown]
"""
## 3. Implementation - Building the Quantization Engine

### Our Implementation Strategy

We'll build quantization in logical layers, each building on the previous:

```
Quantization System Architecture:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Layer 4: Model Quantization             â”‚
â”‚  quantize_model() - Convert entire neural networks         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    Layer 3: Layer Quantization             â”‚
â”‚  QuantizedLinear - Quantized linear transformations        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    Layer 2: Tensor Operations              â”‚
â”‚  quantize_int8() - Core quantization algorithm             â”‚
â”‚  dequantize_int8() - Restore to floating point             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    Layer 1: Foundation                     â”‚
â”‚  Scale & Zero Point Calculation - Parameter optimization   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What We're About to Build

**Core Functions:**
- `quantize_int8()` - Convert FP32 tensors to INT8
- `dequantize_int8()` - Convert INT8 back to FP32
- `QuantizedLinear` - Quantized version of Linear layers
- `quantize_model()` - Quantize entire neural networks

**Key Features:**
- **Automatic calibration** - Find optimal quantization parameters
- **Error minimization** - Preserve accuracy during compression
- **Memory tracking** - Measure actual savings achieved
- **Production patterns** - Industry-standard algorithms

Let's start with the fundamental building block!
"""

# %% [markdown]
"""
### INT8 Quantization - The Foundation

This is the core function that converts any FP32 tensor to INT8. Think of it as a smart compression algorithm that preserves the most important information.

```
Quantization Process Visualization:

Step 1: Analyze Range              Step 2: Calculate Parameters       Step 3: Apply Formula
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input: [-1.5, 0.2, 2.8]    â”‚    â”‚ Min: -1.5               â”‚  â”‚ quantized = round(     â”‚
â”‚                         â”‚    â”‚ Max: 2.8                â”‚  â”‚   (value - zp*scale)   â”‚
â”‚ Find min/max values     â”‚ â†’  â”‚ Range: 4.3              â”‚ â†’â”‚   / scale)             â”‚
â”‚                         â”‚    â”‚ Scale: 4.3/255 = 0.017  â”‚  â”‚                       â”‚
â”‚                         â”‚    â”‚ Zero Point: 88          â”‚  â”‚ Result: [-128, 12, 127] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Challenges This Function Solves:**
- **Dynamic Range:** Each tensor has different min/max values
- **Precision Loss:** Map 4 billion FP32 values to just 256 INT8 values
- **Zero Preservation:** Ensure FP32 zero maps exactly to an INT8 value
- **Symmetric Mapping:** Distribute quantization levels efficiently

**Why This Algorithm:**
- **Linear mapping** preserves relative relationships between values
- **Symmetric quantization** works well for most neural network weights
- **Clipping to [-128, 127]** ensures valid INT8 range
- **Round-to-nearest** minimizes quantization error
"""

# %% nbgrader={"grade": false, "grade_id": "quantize_int8", "solution": true}
def quantize_int8(tensor: Tensor) -> Tuple[Tensor, float, int]:
    """
    Quantize FP32 tensor to INT8 using symmetric quantization.

    TODO: Implement INT8 quantization with scale and zero_point calculation

    APPROACH:
    1. Find min/max values in tensor data
    2. Calculate scale: (max_val - min_val) / 255 (INT8 range: -128 to 127)
    3. Calculate zero_point: offset to map FP32 zero to INT8 zero
    4. Apply quantization formula: round((value - zero_point) / scale)
    5. Clamp to INT8 range [-128, 127]

    EXAMPLE:
    >>> tensor = Tensor([[-1.0, 0.0, 2.0], [0.5, 1.5, -0.5]])
    >>> q_tensor, scale, zero_point = quantize_int8(tensor)
    >>> print(f"Scale: {scale:.4f}, Zero point: {zero_point}")
    Scale: 0.0118, Zero point: 42

    HINTS:
    - Use np.round() for quantization
    - Clamp with np.clip(values, -128, 127)
    - Handle edge case where min_val == max_val (set scale=1.0)
    """
    ### BEGIN SOLUTION
    data = tensor.data

    # Step 1: Find dynamic range
    min_val = float(np.min(data))
    max_val = float(np.max(data))

    # Step 2: Handle edge case (constant tensor)
    if abs(max_val - min_val) < 1e-8:
        scale = 1.0
        zero_point = 0
        quantized_data = np.zeros_like(data, dtype=np.int8)
        return Tensor(quantized_data), scale, zero_point

    # Step 3: Calculate scale and zero_point for standard quantization
    # Map [min_val, max_val] to [-128, 127] (INT8 range)
    scale = (max_val - min_val) / 255.0
    zero_point = int(np.round(-128 - min_val / scale))

    # Clamp zero_point to valid INT8 range
    zero_point = int(np.clip(zero_point, -128, 127))

    # Step 4: Apply quantization formula: q = (x / scale) + zero_point
    quantized_data = np.round(data / scale + zero_point)

    # Step 5: Clamp to INT8 range and convert to int8
    quantized_data = np.clip(quantized_data, -128, 127).astype(np.int8)

    return Tensor(quantized_data), scale, zero_point
    ### END SOLUTION

def test_unit_quantize_int8():
    """ğŸ”¬ Test INT8 quantization implementation."""
    print("ğŸ”¬ Unit Test: INT8 Quantization...")

    # Test basic quantization
    tensor = Tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
    q_tensor, scale, zero_point = quantize_int8(tensor)

    # Verify quantized values are in INT8 range
    assert np.all(q_tensor.data >= -128)
    assert np.all(q_tensor.data <= 127)
    assert isinstance(scale, float)
    assert isinstance(zero_point, int)

    # Test dequantization preserves approximate values
    dequantized = scale * (q_tensor.data - zero_point)
    error = np.mean(np.abs(tensor.data - dequantized))
    assert error < 0.2, f"Quantization error too high: {error}"

    # Test edge case: constant tensor
    constant_tensor = Tensor([[2.0, 2.0], [2.0, 2.0]])
    q_const, scale_const, zp_const = quantize_int8(constant_tensor)
    assert scale_const == 1.0

    print("âœ… INT8 quantization works correctly!")

test_unit_quantize_int8()

# %% [markdown]
"""
### INT8 Dequantization - Restoring Precision

Dequantization is the inverse process - converting compressed INT8 values back to usable FP32. This is where we "decompress" our quantized data.

```
Dequantization Process:

INT8 Values + Parameters â†’ FP32 Reconstruction

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Quantized: [-128, 12, 127]   â”‚
â”‚ Scale: 0.017               â”‚
â”‚ Zero Point: 88             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼ Apply Formula
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FP32 = scale Ã— quantized    â”‚
â”‚        + zero_point Ã— scale â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Result: [-1.496, 0.204, 2.799]â”‚
â”‚ Original: [-1.5, 0.2, 2.8]  â”‚
â”‚ Error: [0.004, 0.004, 0.001] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†‘
  Excellent approximation!
```

**Why This Step Is Critical:**
- **Neural networks expect FP32** - INT8 values would confuse computations
- **Preserves computation compatibility** - works with existing matrix operations
- **Controlled precision loss** - error is bounded and predictable
- **Hardware flexibility** - can use FP32 or specialized INT8 operations

**When Dequantization Happens:**
- **During forward pass** - before matrix multiplications
- **For gradient computation** - during backward pass
- **Educational approach** - production uses INT8 GEMM directly
"""

# %% nbgrader={"grade": false, "grade_id": "dequantize_int8", "solution": true}
def dequantize_int8(q_tensor: Tensor, scale: float, zero_point: int) -> Tensor:
    """
    Dequantize INT8 tensor back to FP32.

    TODO: Implement dequantization using the inverse formula

    APPROACH:
    1. Apply inverse quantization: scale * quantized_value + zero_point * scale
    2. Return as new FP32 Tensor

    EXAMPLE:
    >>> q_tensor = Tensor([[-42, 0, 85]])  # INT8 values
    >>> scale, zero_point = 0.0314, 64
    >>> fp32_tensor = dequantize_int8(q_tensor, scale, zero_point)
    >>> print(fp32_tensor.data)
    [[-1.31, 2.01, 2.67]]  # Approximate original values

    HINT:
    - Formula: dequantized = scale * quantized + zero_point * scale
    """
    ### BEGIN SOLUTION
    # Apply inverse quantization formula
    dequantized_data = scale * q_tensor.data + zero_point * scale
    return Tensor(dequantized_data.astype(np.float32))
    ### END SOLUTION

def test_unit_dequantize_int8():
    """ğŸ”¬ Test INT8 dequantization implementation."""
    print("ğŸ”¬ Unit Test: INT8 Dequantization...")

    # Test round-trip: quantize â†’ dequantize
    original = Tensor([[-1.5, 0.0, 3.2], [1.1, -0.8, 2.7]])
    q_tensor, scale, zero_point = quantize_int8(original)
    restored = dequantize_int8(q_tensor, scale, zero_point)

    # Verify round-trip error is small
    error = np.mean(np.abs(original.data - restored.data))
    assert error < 2.0, f"Round-trip error too high: {error}"

    # Verify output is float32
    assert restored.data.dtype == np.float32

    print("âœ… INT8 dequantization works correctly!")

test_unit_dequantize_int8()

# %% [markdown]
"""
## Quantization Quality - Understanding the Impact

### Why Distribution Matters

Different types of data quantize differently. Let's understand how various weight distributions affect quantization quality.

```
Quantization Quality Factors:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Distribution  â”‚   Scale Usage   â”‚   Error Level   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Uniform         â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚      Low       â”‚
â”‚ Normal          â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â”‚    Medium      â”‚
â”‚ With Outliers   â”‚ â–ˆâ–ˆâ–ˆâ–ˆ             â”‚     High       â”‚
â”‚ Sparse (zeros)  â”‚ â–ˆâ–ˆâ–ˆâ–ˆ             â”‚     High       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Scale Utilization Problem

```
Good Quantization (Uniform):     Bad Quantization (Outliers):

Values: [-1.0 ... +1.0]          Values: [-10.0, -0.1...+0.1, +10.0]
   â†“                                 â†“
INT8: -128 ......... +127         INT8: -128 ... 0 ... +127
       â†‘ â†‘ â†‘ â†‘ â†‘ â†‘ â†‘                  â†‘           â†‘
    All levels used               Most levels wasted!

Scale: 0.0078 (good precision)    Scale: 0.078 (poor precision)
Error: ~0.004                     Error: ~0.04 (10Ã— worse!)
```

**Key Insight:** Outliers waste quantization levels and hurt precision for normal values.
"""

# %% nbgrader={"grade": false, "grade_id": "analyze_quantization_error", "solution": true}
def analyze_quantization_error():
    """ğŸ“Š Analyze quantization error across different distributions."""
    print("ğŸ“Š Analyzing Quantization Error Across Distributions...")

    distributions = {
        'uniform': np.random.uniform(-1, 1, (1000,)),
        'normal': np.random.normal(0, 0.5, (1000,)),
        'outliers': np.concatenate([np.random.normal(0, 0.1, (900,)),
                                   np.random.uniform(-2, 2, (100,))]),
        'sparse': np.random.choice([0, 0, 0, 1], size=(1000,)) * np.random.normal(0, 1, (1000,))
    }

    results = {}

    for name, data in distributions.items():
        # Quantize and measure error
        original = Tensor(data)
        q_tensor, scale, zero_point = quantize_int8(original)
        restored = dequantize_int8(q_tensor, scale, zero_point)

        # Calculate metrics
        mse = np.mean((original.data - restored.data) ** 2)
        max_error = np.max(np.abs(original.data - restored.data))

        results[name] = {
            'mse': mse,
            'max_error': max_error,
            'scale': scale,
            'range_ratio': (np.max(data) - np.min(data)) / scale if scale > 0 else 0
        }

        print(f"{name:8}: MSE={mse:.6f}, Max Error={max_error:.4f}, Scale={scale:.4f}")

    print("\nğŸ’¡ Insights:")
    print("- Uniform: Low error, good scale utilization")
    print("- Normal: Higher error at distribution tails")
    print("- Outliers: Poor quantization due to extreme values")
    print("- Sparse: Wasted quantization levels on zeros")

    return results

# Analyze quantization quality
error_analysis = analyze_quantization_error()

# %% [markdown]
"""
## QuantizedLinear - The Heart of Efficient Networks

### Why We Need Quantized Layers

A quantized model isn't just about storing weights in INT8 - we need layers that can work efficiently with quantized data.

```
Regular Linear Layer:              QuantizedLinear Layer:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input: FP32         â”‚            â”‚ Input: FP32         â”‚
â”‚ Weights: FP32       â”‚            â”‚ Weights: INT8       â”‚
â”‚ Computation: FP32   â”‚    VS      â”‚ Computation: Mixed  â”‚
â”‚ Output: FP32        â”‚            â”‚ Output: FP32        â”‚
â”‚ Memory: 4Ã— more     â”‚            â”‚ Memory: 4Ã— less     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Quantized Forward Pass

```
Quantized Linear Layer Forward Pass:

    Input (FP32)                  Quantized Weights (INT8)
         â”‚                               â”‚
         â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Calibrate    â”‚              â”‚   Dequantize    â”‚
â”‚   (optional)    â”‚              â”‚   Weights       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                               â”‚
         â–¼                               â–¼
    Input (FP32)                  Weights (FP32)
         â”‚                               â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Matrix Multiply â”‚
                â”‚   (FP32 GEMM)   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                   Output (FP32)

Memory Saved: 4Ã— for weights storage!
Speed: Depends on dequantization overhead vs INT8 GEMM support
```

### Calibration - Finding Optimal Input Quantization

```
Calibration Process:

 Step 1: Collect Sample Inputs    Step 2: Analyze Distribution    Step 3: Optimize Parameters
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ input_1: [-0.5, 0.2, ..] â”‚      â”‚   Min: -0.8            â”‚    â”‚ Scale: 0.00627          â”‚
 â”‚ input_2: [-0.3, 0.8, ..] â”‚  â†’   â”‚   Max: +0.8            â”‚ â†’  â”‚ Zero Point: 0           â”‚
 â”‚ input_3: [-0.1, 0.5, ..] â”‚      â”‚   Range: 1.6           â”‚    â”‚ Optimal for this data   â”‚
 â”‚ ...                     â”‚      â”‚   Distribution: Normal  â”‚    â”‚ range and distribution  â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why Calibration Matters:**
- **Without calibration:** Generic quantization parameters may waste precision
- **With calibration:** Parameters optimized for actual data distribution
- **Result:** Better accuracy preservation with same memory savings
"""

# %% [markdown]
"""
### QuantizedLinear Class - Efficient Neural Network Layer

This class replaces regular Linear layers with quantized versions that use 4Ã— less memory while preserving functionality.

```
QuantizedLinear Architecture:

Creation Time:                   Runtime:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Regular Linear Layer      â”‚         â”‚ Input (FP32)            â”‚
â”‚ â†“                       â”‚         â”‚ â†“                     â”‚
â”‚ Quantize weights â†’ INT8  â”‚         â”‚ Optional: quantize inputâ”‚
â”‚ Quantize bias â†’ INT8     â”‚    â†’    â”‚ â†“                     â”‚
â”‚ Store quantization params â”‚         â”‚ Dequantize weights      â”‚
â”‚ Ready for deployment!     â”‚         â”‚ â†“                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ Matrix multiply (FP32)  â”‚
      One-time cost                  â”‚ â†“                     â”‚
                                     â”‚ Output (FP32)           â”‚
                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        Per-inference cost
```

**Key Design Decisions:**

1. **Store original layer reference** - for debugging and comparison
2. **Separate quantization parameters** - weights and bias may need different scales
3. **Calibration support** - optimize input quantization using real data
4. **FP32 computation** - educational approach, production uses INT8 GEMM
5. **Memory tracking** - measure actual compression achieved

**Memory Layout Comparison:**
```
Regular Linear Layer:           QuantizedLinear Layer:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ weights: FP32 Ã— N       â”‚     â”‚ q_weights: INT8 Ã— N    â”‚
â”‚ bias: FP32 Ã— M          â”‚     â”‚ q_bias: INT8 Ã— M       â”‚
â”‚                         â”‚ â†’   â”‚ weight_scale: 1 float   â”‚
â”‚ Total: 4Ã—(N+M) bytes    â”‚     â”‚ weight_zero_point: 1 intâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ bias_scale: 1 float     â”‚
                                  â”‚ bias_zero_point: 1 int  â”‚
                                  â”‚                         â”‚
                                  â”‚ Total: (N+M) + 16 bytes â”‚
                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â†‘
                               ~4Ã— smaller!
```

**Production vs Educational Trade-off:**
- **Our approach:** Dequantize â†’ FP32 computation (easier to understand)
- **Production:** INT8 GEMM operations (faster, more complex)
- **Both achieve:** Same memory savings, similar accuracy
"""

# %% nbgrader={"grade": false, "grade_id": "quantized_linear", "solution": true}
class QuantizedLinear:
    """Quantized version of Linear layer using INT8 arithmetic."""

    def __init__(self, linear_layer: Linear):
        """
        Create quantized version of existing linear layer.

        TODO: Quantize weights and bias, store quantization parameters

        APPROACH:
        1. Quantize weights using quantize_int8
        2. Quantize bias if it exists
        3. Store original layer reference for forward pass
        4. Store quantization parameters for dequantization

        IMPLEMENTATION STRATEGY:
        - Store quantized weights, scales, and zero points
        - Implement forward pass using dequantized computation (educational approach)
        - Production: Would use INT8 matrix multiplication libraries
        """
        ### BEGIN SOLUTION
        self.original_layer = linear_layer

        # Quantize weights
        self.q_weight, self.weight_scale, self.weight_zero_point = quantize_int8(linear_layer.weight)

        # Quantize bias if it exists
        if linear_layer.bias is not None:
            self.q_bias, self.bias_scale, self.bias_zero_point = quantize_int8(linear_layer.bias)
        else:
            self.q_bias = None
            self.bias_scale = None
            self.bias_zero_point = None

        # Store input quantization parameters (set during calibration)
        self.input_scale = None
        self.input_zero_point = None
        ### END SOLUTION

    def calibrate(self, sample_inputs: List[Tensor]):
        """
        Calibrate input quantization parameters using sample data.

        TODO: Calculate optimal input quantization parameters

        APPROACH:
        1. Collect statistics from sample inputs
        2. Calculate optimal scale and zero_point for inputs
        3. Store for use in forward pass
        """
        ### BEGIN SOLUTION
        # Collect all input values
        all_values = []
        for inp in sample_inputs:
            all_values.extend(inp.data.flatten())

        all_values = np.array(all_values)

        # Calculate input quantization parameters
        min_val = float(np.min(all_values))
        max_val = float(np.max(all_values))

        if abs(max_val - min_val) < 1e-8:
            self.input_scale = 1.0
            self.input_zero_point = 0
        else:
            self.input_scale = (max_val - min_val) / 255.0
            self.input_zero_point = int(np.round(-128 - min_val / self.input_scale))
            self.input_zero_point = np.clip(self.input_zero_point, -128, 127)
        ### END SOLUTION

    def forward(self, x: Tensor) -> Tensor:
        """
        Forward pass with quantized computation.

        TODO: Implement quantized forward pass

        APPROACH:
        1. Quantize input (if calibrated)
        2. Dequantize weights and input for computation (educational approach)
        3. Perform matrix multiplication
        4. Return FP32 result

        NOTE: Production quantization uses INT8 GEMM libraries for speed
        """
        ### BEGIN SOLUTION
        # For educational purposes, we dequantize and compute in FP32
        # Production systems use specialized INT8 GEMM operations

        # Dequantize weights
        weight_fp32 = dequantize_int8(self.q_weight, self.weight_scale, self.weight_zero_point)

        # Perform computation (same as original layer)
        result = x.matmul(weight_fp32)

        # Add bias if it exists
        if self.q_bias is not None:
            bias_fp32 = dequantize_int8(self.q_bias, self.bias_scale, self.bias_zero_point)
            result = Tensor(result.data + bias_fp32.data)

        return result
        ### END SOLUTION

    def __call__(self, x: Tensor) -> Tensor:
        """Allows the quantized linear layer to be called like a function."""
        return self.forward(x)

    def parameters(self) -> List[Tensor]:
        """Return quantized parameters."""
        params = [self.q_weight]
        if self.q_bias is not None:
            params.append(self.q_bias)
        return params

    def memory_usage(self) -> Dict[str, float]:
        """Calculate memory usage in bytes."""
        ### BEGIN SOLUTION
        # Original FP32 usage
        original_weight_bytes = self.original_layer.weight.data.size * 4  # 4 bytes per FP32
        original_bias_bytes = 0
        if self.original_layer.bias is not None:
            original_bias_bytes = self.original_layer.bias.data.size * 4

        # Quantized INT8 usage
        quantized_weight_bytes = self.q_weight.data.size * 1  # 1 byte per INT8
        quantized_bias_bytes = 0
        if self.q_bias is not None:
            quantized_bias_bytes = self.q_bias.data.size * 1

        # Add overhead for scales and zero points (small)
        overhead_bytes = 8 * 2  # 2 floats + 2 ints for weight/bias quantization params

        return {
            'original_bytes': original_weight_bytes + original_bias_bytes,
            'quantized_bytes': quantized_weight_bytes + quantized_bias_bytes + overhead_bytes,
            'compression_ratio': (original_weight_bytes + original_bias_bytes) /
                               (quantized_weight_bytes + quantized_bias_bytes + overhead_bytes)
        }
        ### END SOLUTION

def test_unit_quantized_linear():
    """ğŸ”¬ Test QuantizedLinear implementation."""
    print("ğŸ”¬ Unit Test: QuantizedLinear...")

    # Create original linear layer
    original = Linear(4, 3)
    original.weight = Tensor(np.random.randn(4, 3) * 0.5)  # Smaller range for testing
    original.bias = Tensor(np.random.randn(3) * 0.1)

    # Create quantized version
    quantized = QuantizedLinear(original)

    # Test forward pass
    x = Tensor(np.random.randn(2, 4) * 0.5)

    # Original forward pass
    original_output = original.forward(x)

    # Quantized forward pass
    quantized_output = quantized.forward(x)

    # Compare outputs (should be close but not identical due to quantization)
    error = np.mean(np.abs(original_output.data - quantized_output.data))
    assert error < 1.0, f"Quantization error too high: {error}"

    # Test memory usage
    memory_info = quantized.memory_usage()
    assert memory_info['compression_ratio'] > 3.0, "Should achieve ~4Ã— compression"

    print(f"  Memory reduction: {memory_info['compression_ratio']:.1f}Ã—")
    print("âœ… QuantizedLinear works correctly!")

test_unit_quantized_linear()

# %% [markdown]
"""
## 4. Integration - Scaling to Full Neural Networks

### The Model Quantization Challenge

Quantizing individual tensors is useful, but real applications need to quantize entire neural networks with multiple layers, activations, and complex data flows.

```
Model Quantization Process:

Original Model:                    Quantized Model:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Linear(784, 128)    [FP32]  â”‚    â”‚ QuantizedLinear(784, 128)   â”‚
â”‚ ReLU()             [FP32]  â”‚    â”‚ ReLU()             [FP32]   â”‚
â”‚ Linear(128, 64)     [FP32]  â”‚ â†’  â”‚ QuantizedLinear(128, 64)    â”‚
â”‚ ReLU()             [FP32]  â”‚    â”‚ ReLU()             [FP32]   â”‚
â”‚ Linear(64, 10)      [FP32]  â”‚    â”‚ QuantizedLinear(64, 10)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    Memory: 100%                      Memory: ~25%
    Speed: Baseline                   Speed: 2-4Ã— faster
```

### Smart Layer Selection

Not all layers benefit equally from quantization:

```
Layer Quantization Strategy:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer Type      â”‚ Quantize?       â”‚ Reason                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Linear/Dense    â”‚ âœ… YES          â”‚ Most parameters, big savings â”‚
â”‚ Convolution     â”‚ âœ… YES          â”‚ Many weights, good candidate â”‚
â”‚ Embedding       â”‚ âœ… YES          â”‚ Large lookup tables         â”‚
â”‚ ReLU/Sigmoid    â”‚ âŒ NO           â”‚ No parameters to quantize   â”‚
â”‚ BatchNorm       â”‚ ğŸ¤” MAYBE        â”‚ Few params, may hurt        â”‚
â”‚ First Layer     â”‚ ğŸ¤” MAYBE        â”‚ Often sensitive to precision â”‚
â”‚ Last Layer      â”‚ ğŸ¤” MAYBE        â”‚ Output quality critical     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Calibration Data Flow

```
End-to-End Calibration:

Calibration Input                     Layer-by-Layer Processing
     â”‚                                       â”‚
     â–¼                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Sample Data â”‚ â†’ â”‚ Layer 1: Collect activation statistics    â”‚
â”‚ [batch of   â”‚   â”‚          â†“                               â”‚
â”‚  real data] â”‚   â”‚ Layer 2: Collect activation statistics    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚          â†“                               â”‚
                  â”‚ Layer 3: Collect activation statistics    â”‚
                  â”‚          â†“                               â”‚
                  â”‚ Optimize quantization parameters         â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
                              Ready for deployment!
```

### Memory Impact Visualization

```
Model Memory Breakdown:

Before Quantization:          After Quantization:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 1: 3.1MB      â”‚       â”‚ Layer 1: 0.8MB     â”‚ (-75%)
â”‚ Layer 2: 0.5MB      â”‚   â†’   â”‚ Layer 2: 0.1MB     â”‚ (-75%)
â”‚ Layer 3: 0.3MB      â”‚       â”‚ Layer 3: 0.1MB     â”‚ (-75%)
â”‚ Total: 3.9MB        â”‚       â”‚ Total: 1.0MB       â”‚ (-74%)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

 Typical mobile phone memory: 4-8GB
 Model now fits: 4000Ã— more models in memory!
```

Now let's implement the functions that make this transformation possible!
"""

# %% [markdown]
"""
### Model Quantization - Scaling to Full Networks

This function transforms entire neural networks from FP32 to quantized versions. It's like upgrading a whole building to be more energy efficient!

```
Model Transformation Process:

Input Model:                    Quantized Model:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ layers[0]: Linear(784, 128) â”‚    â”‚ layers[0]: QuantizedLinear  â”‚
â”‚ layers[1]: ReLU()           â”‚    â”‚ layers[1]: ReLU()           â”‚
â”‚ layers[2]: Linear(128, 64)  â”‚ â†’  â”‚ layers[2]: QuantizedLinear  â”‚
â”‚ layers[3]: ReLU()           â”‚    â”‚ layers[3]: ReLU()           â”‚
â”‚ layers[4]: Linear(64, 10)   â”‚    â”‚ layers[4]: QuantizedLinear  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   Memory: 100%                      Memory: ~25%
   Interface: Same                   Interface: Identical
```

**Smart Layer Selection Logic:**
```
Quantization Decision Tree:

For each layer in model:
    â”‚
    â”œâ”€â”€ Is it a Linear layer?
    â”‚   â”‚
    â”‚   â””â”€â”€ YES â†’ Replace with QuantizedLinear
    â”‚
    â””â”€â”€ Is it ReLU/Activation?
        â”‚
        â””â”€â”€ NO â†’ Keep unchanged (no parameters to quantize)
```

**Calibration Integration:**
```
Calibration Data Flow:

     Input Data              Layer-by-Layer Processing
         â”‚                            â”‚
         â–¼                            â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Sample Batch 1   â”‚    â”‚ Layer 0: Forward â†’ Collect activation statistics        â”‚
  â”‚ Sample Batch 2   â”‚ â†’  â”‚    â†“                                                 â”‚
  â”‚ ...             â”‚    â”‚ Layer 2: Forward â†’ Collect activation statistics        â”‚
  â”‚ Sample Batch N   â”‚    â”‚    â†“                                                 â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ Layer 4: Forward â†’ Collect activation statistics        â”‚
                         â”‚    â†“                                                 â”‚
                         â”‚ For each layer: calibrate optimal quantization      â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why In-Place Modification:**
- **Preserves model structure** - Same interface, same behavior
- **Memory efficient** - No copying of large tensors
- **Drop-in replacement** - Existing code works unchanged
- **Gradual quantization** - Can selectively quantize sensitive layers

**Deployment Benefits:**
```
Before Quantization:            After Quantization:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âŒ Can't fit on phone      â”‚     â”‚ âœ… Fits on mobile device â”‚
â”‚ âŒ Slow cloud deployment   â”‚     â”‚ âœ… Fast edge inference   â”‚
â”‚ âŒ High memory usage       â”‚ â†’   â”‚ âœ… 4Ã— memory efficiency   â”‚
â”‚ âŒ Expensive to serve      â”‚     â”‚ âœ… Lower serving costs    â”‚
â”‚ âŒ Battery drain           â”‚     â”‚ âœ… Extended battery life  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
"""

# %% nbgrader={"grade": false, "grade_id": "quantize_model", "solution": true}
def quantize_model(model, calibration_data: Optional[List[Tensor]] = None) -> None:
    """
    Quantize all Linear layers in a model in-place.

    TODO: Replace all Linear layers with QuantizedLinear versions

    APPROACH:
    1. Find all Linear layers in the model
    2. Replace each with QuantizedLinear version
    3. If calibration data provided, calibrate input quantization
    4. Handle Sequential containers properly

    EXAMPLE:
    >>> model = Sequential(Linear(10, 5), ReLU(), Linear(5, 2))
    >>> quantize_model(model)
    >>> # Now model uses quantized layers

    HINT:
    - Handle Sequential.layers list for layer replacement
    - Use isinstance(layer, Linear) to identify layers to quantize
    """
    ### BEGIN SOLUTION
    if hasattr(model, 'layers'):  # Sequential model
        for i, layer in enumerate(model.layers):
            if isinstance(layer, Linear):
                # Replace with quantized version
                quantized_layer = QuantizedLinear(layer)

                # Calibrate if data provided
                if calibration_data is not None:
                    # Run forward passes to get intermediate activations
                    sample_inputs = []
                    for data in calibration_data[:10]:  # Use first 10 samples for efficiency
                        # Forward through layers up to this point
                        x = data
                        for j in range(i):
                            if hasattr(model.layers[j], 'forward'):
                                x = model.layers[j].forward(x)
                        sample_inputs.append(x)

                    quantized_layer.calibrate(sample_inputs)

                model.layers[i] = quantized_layer

    elif isinstance(model, Linear):  # Single Linear layer
        # Can't replace in-place for single layer, user should handle
        raise ValueError("Cannot quantize single Linear layer in-place. Use QuantizedLinear directly.")

    else:
        raise ValueError(f"Unsupported model type: {type(model)}")
    ### END SOLUTION

def test_unit_quantize_model():
    """ğŸ”¬ Test model quantization implementation."""
    print("ğŸ”¬ Unit Test: Model Quantization...")

    # Create test model
    model = Sequential(
        Linear(4, 8),
        ReLU(),
        Linear(8, 3)
    )

    # Initialize weights
    model.layers[0].weight = Tensor(np.random.randn(4, 8) * 0.5)
    model.layers[0].bias = Tensor(np.random.randn(8) * 0.1)
    model.layers[2].weight = Tensor(np.random.randn(8, 3) * 0.5)
    model.layers[2].bias = Tensor(np.random.randn(3) * 0.1)

    # Test original model
    x = Tensor(np.random.randn(2, 4))
    original_output = model.forward(x)

    # Create calibration data
    calibration_data = [Tensor(np.random.randn(1, 4)) for _ in range(5)]

    # Quantize model
    quantize_model(model, calibration_data)

    # Verify layers were replaced
    assert isinstance(model.layers[0], QuantizedLinear)
    assert isinstance(model.layers[1], ReLU)  # Should remain unchanged
    assert isinstance(model.layers[2], QuantizedLinear)

    # Test quantized model
    quantized_output = model.forward(x)

    # Compare outputs
    error = np.mean(np.abs(original_output.data - quantized_output.data))
    print(f"  Model quantization error: {error:.4f}")
    assert error < 2.0, f"Model quantization error too high: {error}"

    print("âœ… Model quantization works correctly!")

test_unit_quantize_model()

# %% [markdown]
"""
### Model Size Comparison - Measuring the Impact

This function provides detailed analysis of memory savings achieved through quantization. It's like a before/after comparison for model efficiency.

```
Memory Analysis Framework:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Memory Breakdown Analysis                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Component      â”‚  Original (FP32) â”‚ Quantized (INT8) â”‚  Savings        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer 1 weights â”‚    12.8 MB      â”‚     3.2 MB      â”‚    9.6 MB (75%)â”‚
â”‚ Layer 1 bias    â”‚     0.5 MB      â”‚     0.1 MB      â”‚    0.4 MB (75%)â”‚
â”‚ Layer 2 weights â”‚     2.0 MB      â”‚     0.5 MB      â”‚    1.5 MB (75%)â”‚
â”‚ Layer 2 bias    â”‚     0.3 MB      â”‚     0.1 MB      â”‚    0.2 MB (67%)â”‚
â”‚ Overhead        â”‚     0.0 MB      â”‚     0.02 MB     â”‚   -0.02 MB    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TOTAL           â”‚    15.6 MB      â”‚     3.92 MB     â”‚   11.7 MB (74%)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†‘
                    4Ã— compression ratio!
```

**Comprehensive Metrics Provided:**
```
Output Dictionary:
{
  'original_params': 4000000,        # Total parameter count
  'quantized_params': 4000000,       # Same count, different precision
  'original_bytes': 16000000,        # 4 bytes per FP32 parameter
  'quantized_bytes': 4000016,        # 1 byte per INT8 + overhead
  'compression_ratio': 3.99,         # Nearly 4Ã— compression
  'memory_saved_mb': 11.7,           # Absolute savings in MB
  'memory_saved_percent': 74.9       # Relative savings percentage
}
```

**Why These Metrics Matter:**

**For Developers:**
- **compression_ratio** - How much smaller is the model?
- **memory_saved_mb** - Actual bytes freed up
- **memory_saved_percent** - Efficiency improvement

**For Deployment:**
- **Model fits in device memory?** Check memory_saved_mb
- **Network transfer time?** Reduced by compression_ratio
- **Disk storage savings?** Shown by memory_saved_percent

**For Business:**
- **Cloud costs** reduced by compression_ratio
- **User experience** improved (faster downloads)
- **Device support** expanded (fits on more devices)

**Validation Checks:**
- **Parameter count preservation** - same functionality
- **Reasonable compression ratio** - should be ~4Ã— for INT8
- **Minimal overhead** - quantization parameters are tiny
"""

# %% nbgrader={"grade": false, "grade_id": "compare_model_sizes", "solution": true}
def compare_model_sizes(original_model, quantized_model) -> Dict[str, float]:
    """
    Compare memory usage between original and quantized models.

    TODO: Calculate comprehensive memory comparison

    APPROACH:
    1. Count parameters in both models
    2. Calculate bytes used (FP32 vs INT8)
    3. Include quantization overhead
    4. Return comparison metrics
    """
    ### BEGIN SOLUTION
    # Count original model parameters
    original_params = 0
    original_bytes = 0

    if hasattr(original_model, 'layers'):
        for layer in original_model.layers:
            if hasattr(layer, 'parameters'):
                params = layer.parameters()
                for param in params:
                    original_params += param.data.size
                    original_bytes += param.data.size * 4  # 4 bytes per FP32

    # Count quantized model parameters
    quantized_params = 0
    quantized_bytes = 0

    if hasattr(quantized_model, 'layers'):
        for layer in quantized_model.layers:
            if isinstance(layer, QuantizedLinear):
                memory_info = layer.memory_usage()
                quantized_bytes += memory_info['quantized_bytes']
                params = layer.parameters()
                for param in params:
                    quantized_params += param.data.size
            elif hasattr(layer, 'parameters'):
                # Non-quantized layers
                params = layer.parameters()
                for param in params:
                    quantized_params += param.data.size
                    quantized_bytes += param.data.size * 4

    compression_ratio = original_bytes / quantized_bytes if quantized_bytes > 0 else 1.0
    memory_saved = original_bytes - quantized_bytes

    return {
        'original_params': original_params,
        'quantized_params': quantized_params,
        'original_bytes': original_bytes,
        'quantized_bytes': quantized_bytes,
        'compression_ratio': compression_ratio,
        'memory_saved_mb': memory_saved / (1024 * 1024),
        'memory_saved_percent': (memory_saved / original_bytes) * 100 if original_bytes > 0 else 0
    }
    ### END SOLUTION

def test_unit_compare_model_sizes():
    """ğŸ”¬ Test model size comparison."""
    print("ğŸ”¬ Unit Test: Model Size Comparison...")

    # Create and quantize a model for testing
    original_model = Sequential(Linear(100, 50), ReLU(), Linear(50, 10))
    original_model.layers[0].weight = Tensor(np.random.randn(100, 50))
    original_model.layers[0].bias = Tensor(np.random.randn(50))
    original_model.layers[2].weight = Tensor(np.random.randn(50, 10))
    original_model.layers[2].bias = Tensor(np.random.randn(10))

    # Create quantized copy
    quantized_model = Sequential(Linear(100, 50), ReLU(), Linear(50, 10))
    quantized_model.layers[0].weight = Tensor(np.random.randn(100, 50))
    quantized_model.layers[0].bias = Tensor(np.random.randn(50))
    quantized_model.layers[2].weight = Tensor(np.random.randn(50, 10))
    quantized_model.layers[2].bias = Tensor(np.random.randn(10))

    quantize_model(quantized_model)

    # Compare sizes
    comparison = compare_model_sizes(original_model, quantized_model)

    # Verify compression achieved
    assert comparison['compression_ratio'] > 2.0, "Should achieve significant compression"
    assert comparison['memory_saved_percent'] > 50, "Should save >50% memory"

    print(f"  Compression ratio: {comparison['compression_ratio']:.1f}Ã—")
    print(f"  Memory saved: {comparison['memory_saved_percent']:.1f}%")
    print("âœ… Model size comparison works correctly!")

test_unit_compare_model_sizes()

# %% [markdown]
"""
## 5. Systems Analysis - Real-World Performance Impact

### Understanding Production Trade-offs

Quantization isn't just about smaller models - it's about enabling entirely new deployment scenarios. Let's measure the real impact across different model scales.

```
Production Deployment Scenarios:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Deployment      â”‚   Memory Limit   â”‚   Speed Needs    â”‚ Quantization Fit â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Mobile Phone     â”‚ 100-500MB        â”‚ <100ms latency   â”‚ âœ… Essential     â”‚
â”‚ Edge Device      â”‚ 50-200MB         â”‚ Real-time        â”‚ âœ… Critical      â”‚
â”‚ Cloud GPU        â”‚ 16-80GB          â”‚ High throughput  â”‚ ğŸ¤” Optional      â”‚
â”‚ Embedded MCU     â”‚ 1-10MB           â”‚ Ultra-low power  â”‚ âœ… Mandatory     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Performance Testing Framework

We'll measure quantization impact across three critical dimensions:

```
Performance Analysis Framework:

1. Memory Efficiency                2. Inference Speed               3. Accuracy Preservation
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢ Model size (MB)   â”‚            â”‚ â€¢ Forward pass time â”‚          â”‚ â€¢ MSE vs original   â”‚
â”‚ â€¢ Compression ratio â”‚            â”‚ â€¢ Throughput (fps)  â”‚          â”‚ â€¢ Relative error    â”‚
â”‚ â€¢ Memory bandwidth  â”‚            â”‚ â€¢ Latency (ms)      â”‚          â”‚ â€¢ Distribution      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Expected Results Preview

```
Typical Quantization Results:

Model Size:     Small (1-10MB)      Medium (10-100MB)     Large (100MB+)
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Compression:   â”‚ 3.8Ã— reduction  â”‚  â”‚ 3.9Ã— reduction  â”‚   â”‚ 4.0Ã— reduction  â”‚
Speed:         â”‚ 1.2Ã— faster    â”‚  â”‚ 2.1Ã— faster    â”‚   â”‚ 3.2Ã— faster     â”‚
Accuracy:      â”‚ 0.1% loss      â”‚  â”‚ 0.3% loss      â”‚   â”‚ 0.5% loss       â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Key Insight: Larger models benefit more from quantization!
```

Let's run comprehensive tests to validate these expectations and understand the underlying patterns.
"""

# %% [markdown]
"""
### Performance Analysis - Real-World Benchmarking

This comprehensive analysis measures quantization impact across the three critical dimensions: memory, speed, and accuracy.

```
Performance Testing Strategy:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           Test Model Configurations                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      Model Type        â”‚     Architecture       â”‚      Use Case         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Small MLP            â”‚ 64 â†’ 32 â†’ 10         â”‚ Edge Device          â”‚
â”‚ Medium MLP           â”‚ 512 â†’ 256 â†’ 128 â†’ 10 â”‚ Mobile App           â”‚
â”‚ Large MLP            â”‚ 2048 â†’ 1024 â†’ 512 â†’ 10â”‚ Server Deployment    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Performance Measurement Pipeline:**
```
For Each Model Configuration:

  Create Original Model    Create Quantized Model     Comparative Analysis
         â”‚                        â”‚                        â”‚
         â–¼                        â–¼                        â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Initialize weights  â”‚  â”‚ Copy weights      â”‚  â”‚ Memory analysis   â”‚
  â”‚ Random test data   â”‚  â”‚ Apply quantizationâ”‚  â”‚ Speed benchmarks  â”‚
  â”‚ Forward pass       â”‚  â”‚ Calibrate layers  â”‚  â”‚ Accuracy testing  â”‚
  â”‚ Timing measurementsâ”‚  â”‚ Forward pass      â”‚  â”‚ Trade-off analysisâ”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Expected Performance Patterns:**
```
Model Scaling Effects:

   Memory Usage               Inference Speed              Accuracy Loss
        â”‚                           â”‚                           â”‚
        â–¼                           â–¼                           â–¼

4Ã— â”‚ ############### FP32     3Ã— â”‚                   INT8   1% â”‚ ####
    â”‚                          â”‚ ############### FP32        â”‚
3Ã— â”‚                       2Ã— â”‚                       0.5% â”‚ ##
    â”‚ ######### INT8           â”‚ ########### INT8             â”‚
2Ã— â”‚                       1Ã— â”‚                       0.1% â”‚ #
    â”‚                          â”‚ #######                      â”‚
1Ã— â”‚                          â”‚                           0% â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    Small   Medium   Large
    Small   Medium   Large    Small   Medium   Large

Key Insight: Larger models benefit more from quantization!
```

**Real-World Impact Translation:**
- **Memory savings** â†’ More models fit on device, lower cloud costs
- **Speed improvements** â†’ Better user experience, real-time applications
- **Accuracy preservation** â†’ Maintains model quality, no retraining needed
"""

# %% nbgrader={"grade": false, "grade_id": "analyze_quantization_performance", "solution": true}
def analyze_quantization_performance():
    """ğŸ“Š Comprehensive analysis of quantization benefits and trade-offs."""
    print("ğŸ“Š Analyzing Quantization Performance Across Model Sizes...")

    # Test different model configurations
    configs = [
        {'name': 'Small MLP', 'layers': [64, 32, 10], 'batch_size': 32},
        {'name': 'Medium MLP', 'layers': [512, 256, 128, 10], 'batch_size': 64},
        {'name': 'Large MLP', 'layers': [2048, 1024, 512, 10], 'batch_size': 128},
    ]

    results = []

    for config in configs:
        print(f"\nğŸ” Testing {config['name']}...")

        # Create original model
        layers = []
        for i in range(len(config['layers']) - 1):
            layers.append(Linear(config['layers'][i], config['layers'][i+1]))
            if i < len(config['layers']) - 2:  # Add ReLU except for last layer
                layers.append(ReLU())

        original_model = Sequential(*layers)

        # Initialize weights
        for layer in original_model.layers:
            if isinstance(layer, Linear):
                layer.weight = Tensor(np.random.randn(*layer.weight.shape) * 0.1)
                layer.bias = Tensor(np.random.randn(*layer.bias.shape) * 0.01)

        # Create quantized copy
        quantized_model = Sequential(*layers)
        for i, layer in enumerate(original_model.layers):
            if isinstance(layer, Linear):
                quantized_model.layers[i].weight = Tensor(layer.weight.data.copy())
                quantized_model.layers[i].bias = Tensor(layer.bias.data.copy())

        # Generate calibration data
        input_size = config['layers'][0]
        calibration_data = [Tensor(np.random.randn(1, input_size)) for _ in range(10)]

        # Quantize model
        quantize_model(quantized_model, calibration_data)

        # Measure performance
        test_input = Tensor(np.random.randn(config['batch_size'], input_size))

        # Time original model
        start_time = time.time()
        for _ in range(10):
            original_output = original_model.forward(test_input)
        original_time = (time.time() - start_time) / 10

        # Time quantized model
        start_time = time.time()
        for _ in range(10):
            quantized_output = quantized_model.forward(test_input)
        quantized_time = (time.time() - start_time) / 10

        # Calculate accuracy preservation (using MSE as proxy)
        mse = np.mean((original_output.data - quantized_output.data) ** 2)
        relative_error = np.sqrt(mse) / (np.std(original_output.data) + 1e-8)

        # Memory comparison
        memory_comparison = compare_model_sizes(original_model, quantized_model)

        result = {
            'name': config['name'],
            'original_time': original_time * 1000,  # Convert to ms
            'quantized_time': quantized_time * 1000,
            'speedup': original_time / quantized_time if quantized_time > 0 else 1.0,
            'compression_ratio': memory_comparison['compression_ratio'],
            'relative_error': relative_error,
            'memory_saved_mb': memory_comparison['memory_saved_mb']
        }

        results.append(result)

        print(f"  Speedup: {result['speedup']:.1f}Ã—")
        print(f"  Compression: {result['compression_ratio']:.1f}Ã—")
        print(f"  Error: {result['relative_error']:.1%}")
        print(f"  Memory saved: {result['memory_saved_mb']:.1f}MB")

    # Summary analysis
    print(f"\nğŸ“ˆ QUANTIZATION PERFORMANCE SUMMARY")
    print("=" * 50)

    avg_speedup = np.mean([r['speedup'] for r in results])
    avg_compression = np.mean([r['compression_ratio'] for r in results])
    avg_error = np.mean([r['relative_error'] for r in results])
    total_memory_saved = sum([r['memory_saved_mb'] for r in results])

    print(f"Average speedup: {avg_speedup:.1f}Ã—")
    print(f"Average compression: {avg_compression:.1f}Ã—")
    print(f"Average relative error: {avg_error:.1%}")
    print(f"Total memory saved: {total_memory_saved:.1f}MB")

    print(f"\nğŸ’¡ Key Insights:")
    print(f"- Quantization achieves ~{avg_compression:.0f}Ã— memory reduction")
    print(f"- Typical speedup: {avg_speedup:.1f}Ã— (varies by hardware)")
    print(f"- Accuracy loss: <{avg_error:.1%} for well-calibrated models")
    print(f"- Best for: Memory-constrained deployment")

    return results

# Run comprehensive performance analysis
performance_results = analyze_quantization_performance()

# %% [markdown]
"""
## Quantization Error Visualization - Seeing the Impact

### Understanding Distribution Effects

Different weight distributions quantize with varying quality. Let's visualize this to understand when quantization works well and when it struggles.

```
Visualization Strategy:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Weight Distribution Analysis                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Distribution Type  â”‚  Expected Quality   â”‚         Key Challenge            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Normal (Gaussian)   â”‚ Good                â”‚ Tail values may be clipped      â”‚
â”‚ Uniform             â”‚ Excellent           â”‚ Perfect scale utilization       â”‚
â”‚ Sparse (many zeros) â”‚ Poor                â”‚ Wasted quantization levels      â”‚
â”‚ Heavy-tailed        â”‚ Very Poor           â”‚ Outliers dominate scale         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Quantization Quality Patterns

```
Ideal Quantization:                 Problematic Quantization:

Original: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]     Original: [â–ˆâ–ˆ    â–ˆâ–ˆâ–ˆâ–ˆ    â–ˆâ–ˆ]
              â†“                                   â†“
Quantized: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]     Quantized: [â–ˆâ–ˆ....â–ˆâ–ˆâ–ˆâ–ˆ....â–ˆâ–ˆ]
          Perfect reconstruction              Lost precision

Scale efficiently used             Scale poorly used
Low quantization error             High quantization error
```

**What We'll Visualize:**
- **Before/After histograms** - See how distributions change
- **Error metrics** - Quantify the precision loss
- **Scale utilization** - Understand efficiency
- **Real examples** - Connect to practical scenarios

This visualization will help you understand which types of neural network weights quantize well and which need special handling.
"""

# %% [markdown]
r"""
### Quantization Effects Visualization - Understanding Distribution Impact

This visualization reveals how different weight distributions respond to quantization, helping you understand when quantization works well and when it struggles.

```
Visualization Strategy:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Distribution Analysis Grid                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    Normal (Good)    â”‚   Uniform (Best)    â”‚   Sparse (Bad)     â”‚ Heavy-Tailed (Worst)â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚       /\          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  |     |  |     â”‚       /\            â”‚
â”‚      /  \         â”‚  â”‚          â”‚   â”‚  |     |  |     â”‚      /  \  /\       â”‚
â”‚     /    \        â”‚  â”‚  Flat    â”‚   â”‚  ||||  |  ||||  â”‚     /    \/  \      â”‚
â”‚    /      \       â”‚  â”‚          â”‚   â”‚  zeros    sparse â”‚    /          \     â”‚
â”‚   /        \      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  values         â”‚   /     huge    \    â”‚
â”‚  /          \     â”‚                  â”‚                 â”‚  /     outliers   \   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MSE: 0.001        â”‚ MSE: 0.0001       â”‚ MSE: 0.01        â”‚ MSE: 0.1            â”‚
â”‚ Scale Usage: 80%  â”‚ Scale Usage: 100% â”‚ Scale Usage: 10% â”‚ Scale Usage: 5%     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Visual Comparison Strategy:**
```
For Each Distribution Type:
  â”‚
  â”œâ”€â”€ Generate sample weights (1000 values)
  â”‚
  â”œâ”€â”€ Quantize to INT8
  â”‚
  â”œâ”€â”€ Dequantize back to FP32
  â”‚
  â”œâ”€â”€ Plot overlaid histograms:
  â”‚   â”œâ”€â”€ Original distribution (blue)
  â”‚   â””â”€â”€ Quantized distribution (red)
  â”‚
  â””â”€â”€ Calculate and display error metrics:
      â”œâ”€â”€ Mean Squared Error (MSE)
      â”œâ”€â”€ Scale utilization efficiency
      â””â”€â”€ Quantization scale value
```

**Key Insights You'll Discover:**

**1. Normal Distribution (Most Common):**
   - Smooth bell curve preserved reasonably well
   - Tail values may be clipped slightly
   - Good compromise for most neural networks

**2. Uniform Distribution (Ideal Case):**
   - Perfect scale utilization
   - Minimal quantization error
   - Best-case scenario for quantization

**3. Sparse Distribution (Problematic):**
   - Many zeros waste quantization levels
   - Poor precision for non-zero values
   - Common in pruned networks

**4. Heavy-Tailed Distribution (Worst Case):**
   - Outliers dominate scale calculation
   - Most values squeezed into narrow range
   - Requires special handling (clipping, per-channel)

**Practical Implications:**
- **Model design:** Prefer batch normalization to reduce outliers
- **Training:** Techniques to encourage uniform weight distributions
- **Deployment:** Advanced quantization for sparse/heavy-tailed weights
"""

# %% nbgrader={"grade": false, "grade_id": "visualize_quantization_effects", "solution": true}
def visualize_quantization_effects():
    """ğŸ“Š Visualize the effects of quantization on weight distributions."""
    print("ğŸ“Š Visualizing Quantization Effects on Weight Distributions...")

    # Create sample weight tensors with different characteristics
    weight_types = {
        'Normal': np.random.normal(0, 0.1, (1000,)),
        'Uniform': np.random.uniform(-0.2, 0.2, (1000,)),
        'Sparse': np.random.choice([0, 0, 0, 1], (1000,)) * np.random.normal(0, 0.15, (1000,)),
        'Heavy-tailed': np.concatenate([
            np.random.normal(0, 0.05, (800,)),
            np.random.uniform(-0.5, 0.5, (200,))
        ])
    }

    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    axes = axes.flatten()

    for idx, (name, weights) in enumerate(weight_types.items()):
        # Original weights
        original_tensor = Tensor(weights)

        # Quantize and dequantize
        q_tensor, scale, zero_point = quantize_int8(original_tensor)
        restored_tensor = dequantize_int8(q_tensor, scale, zero_point)

        # Plot histograms
        ax = axes[idx]
        ax.hist(weights, bins=50, alpha=0.6, label='Original', density=True)
        ax.hist(restored_tensor.data, bins=50, alpha=0.6, label='Quantized', density=True)
        ax.set_title(f'{name} Weights\nScale: {scale:.4f}')
        ax.set_xlabel('Weight Value')
        ax.set_ylabel('Density')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # Calculate and display error metrics
        mse = np.mean((weights - restored_tensor.data) ** 2)
        ax.text(0.02, 0.98, f'MSE: {mse:.6f}', transform=ax.transAxes,
                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

    plt.tight_layout()
    plt.savefig('/tmp/claude/quantization_effects.png', dpi=100, bbox_inches='tight')
    plt.show()

    print("ğŸ’¡ Observations:")
    print("- Normal: Smooth quantization, good preservation")
    print("- Uniform: Excellent quantization, full range utilized")
    print("- Sparse: Many wasted quantization levels on zeros")
    print("- Heavy-tailed: Outliers dominate scale, poor precision for small weights")

# Visualize quantization effects
visualize_quantization_effects()

# %% [markdown]
"""
## 6. Optimization Insights - Production Quantization Strategies

### Beyond Basic Quantization

Our INT8 per-tensor quantization is just the beginning. Production systems use sophisticated strategies to squeeze out every bit of performance while preserving accuracy.

```
Quantization Strategy Evolution:

 Basic (What we built)          Advanced (Production)          Cutting-Edge (Research)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢ Per-tensor scale  â”‚        â”‚ â€¢ Per-channel scale â”‚       â”‚ â€¢ Dynamic ranges    â”‚
â”‚ â€¢ Uniform INT8      â”‚   â†’    â”‚ â€¢ Mixed precision   â”‚   â†’   â”‚ â€¢ Adaptive bitwidth â”‚
â”‚ â€¢ Post-training     â”‚        â”‚ â€¢ Quantization-awareâ”‚       â”‚ â€¢ Learned quantizersâ”‚
â”‚ â€¢ Simple calibrationâ”‚        â”‚ â€¢ Advanced calib.   â”‚       â”‚ â€¢ Neural compressionâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Good baseline              Production systems           Future research
```

### Strategy Comparison Framework

```
Quantization Strategy Trade-offs:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Strategy        â”‚  Accuracy   â”‚ Complexity  â”‚ Memory Use  â”‚ Speed Gain  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Per-Tensor (Ours)   â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘  â”‚ â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘  â”‚
â”‚ Per-Channel         â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘  â”‚
â”‚ Mixed Precision     â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘  â”‚
â”‚ Quantization-Aware  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Three Advanced Strategies We'll Analyze

**1. Per-Channel Quantization:**
```
Per-Tensor:                     Per-Channel:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [Wâ‚â‚ Wâ‚â‚‚ Wâ‚â‚ƒ]          â”‚     â”‚ [Wâ‚â‚ Wâ‚â‚‚ Wâ‚â‚ƒ]  scaleâ‚  â”‚
â”‚ [Wâ‚‚â‚ Wâ‚‚â‚‚ Wâ‚‚â‚ƒ] scale    â”‚ VS  â”‚ [Wâ‚‚â‚ Wâ‚‚â‚‚ Wâ‚‚â‚ƒ]  scaleâ‚‚  â”‚
â”‚ [Wâ‚ƒâ‚ Wâ‚ƒâ‚‚ Wâ‚ƒâ‚ƒ]          â”‚     â”‚ [Wâ‚ƒâ‚ Wâ‚ƒâ‚‚ Wâ‚ƒâ‚ƒ]  scaleâ‚ƒ  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    One scale for all           Separate scale per channel
  May waste precision           Better precision per channel
```

**2. Mixed Precision:**
```
Sensitive Layers (FP32):        Regular Layers (INT8):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input Layer             â”‚     â”‚ Hidden Layer 1          â”‚
â”‚ (preserve input quality)â”‚     â”‚ (can tolerate error)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Output Layer            â”‚     â”‚ Hidden Layer 2          â”‚
â”‚ (preserve output)       â”‚     â”‚ (bulk of computation)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Keep high precision         Maximize compression
```

**3. Calibration Strategies:**
```
Basic Calibration:              Advanced Calibration:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢ Use min/max range     â”‚     â”‚ â€¢ Percentile clipping   â”‚
â”‚ â€¢ Simple statistics     â”‚     â”‚ â€¢ KL-divergence         â”‚
â”‚ â€¢ Few samples           â”‚ VS  â”‚ â€¢ Multiple datasets     â”‚
â”‚ â€¢ Generic approach      â”‚     â”‚ â€¢ Layer-specific tuning â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   Fast but suboptimal          Optimal but expensive
```

Let's implement and compare these strategies to understand their practical trade-offs!
"""

# %% [markdown]
"""
### Advanced Quantization Strategies - Production Techniques

This analysis compares different quantization approaches used in production systems, revealing the trade-offs between accuracy, complexity, and performance.

```
Strategy Comparison Framework:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           Three Advanced Strategies                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      Strategy 1       â”‚      Strategy 2       â”‚      Strategy 3       â”‚
â”‚   Per-Tensor (Ours)   â”‚   Per-Channel Scale   â”‚   Mixed Precision     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                        â”‚                        â”‚                        â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Weights:           â”‚ â”‚ â”‚ Channel 1: scaleâ‚  â”‚ â”‚ â”‚ Sensitive: FP32    â”‚ â”‚
â”‚ â”‚ [Wâ‚â‚ Wâ‚â‚‚ Wâ‚â‚ƒ]       â”‚ â”‚ â”‚ Channel 2: scaleâ‚‚  â”‚ â”‚ â”‚ Regular: INT8      â”‚ â”‚
â”‚ â”‚ [Wâ‚‚â‚ Wâ‚‚â‚‚ Wâ‚‚â‚ƒ] scale â”‚ â”‚ â”‚ Channel 3: scaleâ‚ƒ  â”‚ â”‚ â”‚                    â”‚ â”‚
â”‚ â”‚ [Wâ‚ƒâ‚ Wâ‚ƒâ‚‚ Wâ‚ƒâ‚ƒ]       â”‚ â”‚ â”‚                    â”‚ â”‚ â”‚ Input: FP32        â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ Better precision   â”‚ â”‚ â”‚ Output: FP32       â”‚ â”‚
â”‚                        â”‚ â”‚ per channel        â”‚ â”‚ â”‚ Hidden: INT8       â”‚ â”‚
â”‚ Simple, fast          â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ Good baseline         â”‚                        â”‚                        â”‚
â”‚                        â”‚ More complex           â”‚ Optimal accuracy       â”‚
â”‚                        â”‚ Better accuracy        â”‚ Selective compression  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Strategy 1: Per-Tensor Quantization (Our Implementation)**
```
Weight Matrix:                Scale Calculation:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 0.1 -0.3  0.8  0.2      â”‚     â”‚ Global min: -0.5        â”‚
â”‚-0.2  0.5 -0.1  0.7      â”‚ â†’   â”‚ Global max: +0.8        â”‚
â”‚ 0.4 -0.5  0.3 -0.4      â”‚     â”‚ Scale: 1.3/255 = 0.0051 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Pros: Simple, fast           Cons: May waste precision
```

**Strategy 2: Per-Channel Quantization (Advanced)**
```
Weight Matrix:                Scale Calculation:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 0.1 -0.3  0.8  0.2      â”‚     â”‚ Col 1: [-0.2,0.4] â†’ sâ‚  â”‚
â”‚-0.2  0.5 -0.1  0.7      â”‚ â†’   â”‚ Col 2: [-0.5,0.5] â†’ sâ‚‚  â”‚
â”‚ 0.4 -0.5  0.3 -0.4      â”‚     â”‚ Col 3: [-0.1,0.8] â†’ sâ‚ƒ  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ Col 4: [-0.4,0.7] â†’ sâ‚„  â”‚
                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Pros: Better precision       Cons: More complex
```

**Strategy 3: Mixed Precision (Production)**
```
Model Architecture:            Precision Assignment:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input Layer  (sensitive) â”‚     â”‚ Keep in FP32 (precision) â”‚
â”‚ Hidden 1     (bulk)     â”‚ â†’   â”‚ Quantize to INT8        â”‚
â”‚ Hidden 2     (bulk)     â”‚     â”‚ Quantize to INT8        â”‚
â”‚ Output Layer (sensitive)â”‚     â”‚ Keep in FP32 (quality)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Pros: Optimal trade-off      Cons: Requires expertise
```

**Experimental Design:**
```
Comparative Testing Protocol:

1. Create identical test model   â†’  2. Apply each strategy        â†’  3. Measure results
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ 128 â†’ 64 â†’ 10 MLP      â”‚     â”‚ Per-tensor quantization â”‚     â”‚ MSE error calculation  â”‚
   â”‚ Identical weights       â”‚     â”‚ Per-channel simulation  â”‚     â”‚ Compression measurementâ”‚
   â”‚ Same test input         â”‚     â”‚ Mixed precision setup   â”‚     â”‚ Speed comparison       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Expected Strategy Rankings:**
1. **Mixed Precision** - Best accuracy, moderate complexity
2. **Per-Channel** - Good accuracy, higher complexity
3. **Per-Tensor** - Baseline accuracy, simplest implementation

This analysis reveals which strategies work best for different deployment scenarios and accuracy requirements.
"""

# %% nbgrader={"grade": false, "grade_id": "analyze_quantization_strategies", "solution": true}
def analyze_quantization_strategies():
    """ğŸ“Š Compare different quantization strategies and their trade-offs."""
    print("ğŸ“Š Analyzing Advanced Quantization Strategies...")

    # Create test model and data
    model = Sequential(Linear(128, 64), ReLU(), Linear(64, 10))
    model.layers[0].weight = Tensor(np.random.randn(128, 64) * 0.1)
    model.layers[0].bias = Tensor(np.random.randn(64) * 0.01)
    model.layers[2].weight = Tensor(np.random.randn(64, 10) * 0.1)
    model.layers[2].bias = Tensor(np.random.randn(10) * 0.01)

    test_input = Tensor(np.random.randn(32, 128))
    original_output = model.forward(test_input)

    strategies = {}

    # Strategy 1: Per-tensor quantization (what we implemented)
    print("\nğŸ” Strategy 1: Per-Tensor Quantization")
    model_copy = Sequential(Linear(128, 64), ReLU(), Linear(64, 10))
    for i, layer in enumerate(model.layers):
        if isinstance(layer, Linear):
            model_copy.layers[i].weight = Tensor(layer.weight.data.copy())
            model_copy.layers[i].bias = Tensor(layer.bias.data.copy())

    quantize_model(model_copy)
    output1 = model_copy.forward(test_input)
    error1 = np.mean((original_output.data - output1.data) ** 2)
    strategies['per_tensor'] = {'mse': error1, 'description': 'Single scale per tensor'}
    print(f"  MSE: {error1:.6f}")

    # Strategy 2: Per-channel quantization simulation
    print("\nğŸ” Strategy 2: Per-Channel Quantization (simulated)")
    # Simulate by quantizing each output channel separately
    def per_channel_quantize(tensor):
        """Simulate per-channel quantization for 2D weight matrices."""
        if len(tensor.shape) < 2:
            return quantize_int8(tensor)

        quantized_data = np.zeros_like(tensor.data, dtype=np.int8)
        scales = []
        zero_points = []

        for i in range(tensor.shape[1]):  # Per output channel
            channel_tensor = Tensor(tensor.data[:, i:i+1])
            q_channel, scale, zp = quantize_int8(channel_tensor)
            quantized_data[:, i] = q_channel.data.flatten()
            scales.append(scale)
            zero_points.append(zp)

        return Tensor(quantized_data), scales, zero_points

    # Apply per-channel quantization to weights
    total_error = 0
    for layer in model.layers:
        if isinstance(layer, Linear):
            q_weight, scales, zps = per_channel_quantize(layer.weight)
            # Simulate dequantization and error
            for i in range(layer.weight.shape[1]):
                original_channel = layer.weight.data[:, i]
                restored_channel = scales[i] * q_weight.data[:, i] + zps[i] * scales[i]
                total_error += np.mean((original_channel - restored_channel) ** 2)

    strategies['per_channel'] = {'mse': total_error, 'description': 'Scale per output channel'}
    print(f"  MSE: {total_error:.6f}")

    # Strategy 3: Mixed precision simulation
    print("\nğŸ” Strategy 3: Mixed Precision")
    # Keep sensitive layers in FP32, quantize others
    sensitive_layers = [0]  # First layer often most sensitive
    mixed_error = 0

    for i, layer in enumerate(model.layers):
        if isinstance(layer, Linear):
            if i in sensitive_layers:
                # Keep in FP32 (no quantization error)
                pass
            else:
                # Quantize layer
                q_weight, scale, zp = quantize_int8(layer.weight)
                restored = dequantize_int8(q_weight, scale, zp)
                mixed_error += np.mean((layer.weight.data - restored.data) ** 2)

    strategies['mixed_precision'] = {'mse': mixed_error, 'description': 'FP32 sensitive + INT8 others'}
    print(f"  MSE: {mixed_error:.6f}")

    # Compare strategies
    print(f"\nğŸ“Š QUANTIZATION STRATEGY COMPARISON")
    print("=" * 60)
    for name, info in strategies.items():
        print(f"{name:15}: MSE={info['mse']:.6f} | {info['description']}")

    # Find best strategy
    best_strategy = min(strategies.items(), key=lambda x: x[1]['mse'])
    print(f"\nğŸ† Best Strategy: {best_strategy[0]} (MSE: {best_strategy[1]['mse']:.6f})")

    print(f"\nğŸ’¡ Production Insights:")
    print("- Per-channel: Better accuracy, more complex implementation")
    print("- Mixed precision: Optimal accuracy/efficiency trade-off")
    print("- Per-tensor: Simplest, good for most applications")
    print("- Hardware support varies: INT8 GEMM, per-channel scales")

    return strategies

# Analyze quantization strategies
strategy_analysis = analyze_quantization_strategies()

# %% [markdown]
"""
## 7. Module Integration Test

Final validation that our quantization system works correctly across all components.
"""

# %% nbgrader={"grade": true, "grade_id": "test_module", "points": 20}
def test_module():
    """
    Comprehensive test of entire quantization module functionality.

    This final test runs before module summary to ensure:
    - All quantization functions work correctly
    - Model quantization preserves functionality
    - Memory savings are achieved
    - Module is ready for integration with TinyTorch
    """
    print("ğŸ§ª RUNNING MODULE INTEGRATION TEST")
    print("=" * 50)

    # Run all unit tests
    print("Running unit tests...")
    test_unit_quantize_int8()
    test_unit_dequantize_int8()
    test_unit_quantized_linear()
    test_unit_quantize_model()
    test_unit_compare_model_sizes()

    print("\nRunning integration scenarios...")

    # Test realistic usage scenario
    print("ğŸ”¬ Integration Test: End-to-end quantization workflow...")

    # Create a realistic model
    model = Sequential(
        Linear(784, 128),  # MNIST-like input
        ReLU(),
        Linear(128, 64),
        ReLU(),
        Linear(64, 10)     # 10-class output
    )

    # Initialize with realistic weights
    for layer in model.layers:
        if isinstance(layer, Linear):
            # Xavier initialization
            fan_in, fan_out = layer.weight.shape
            std = np.sqrt(2.0 / (fan_in + fan_out))
            layer.weight = Tensor(np.random.randn(fan_in, fan_out) * std)
            layer.bias = Tensor(np.zeros(fan_out))

    # Generate realistic calibration data
    calibration_data = [Tensor(np.random.randn(1, 784) * 0.1) for _ in range(20)]

    # Test original model
    test_input = Tensor(np.random.randn(8, 784) * 0.1)
    original_output = model.forward(test_input)

    # Quantize the model
    quantize_model(model, calibration_data)

    # Test quantized model
    quantized_output = model.forward(test_input)

    # Verify functionality is preserved
    assert quantized_output.shape == original_output.shape, "Output shape mismatch"

    # Verify reasonable accuracy preservation
    mse = np.mean((original_output.data - quantized_output.data) ** 2)
    relative_error = np.sqrt(mse) / (np.std(original_output.data) + 1e-8)
    assert relative_error < 0.1, f"Accuracy degradation too high: {relative_error:.3f}"

    # Verify memory savings
    # Create equivalent original model for comparison
    original_model = Sequential(
        Linear(784, 128),
        ReLU(),
        Linear(128, 64),
        ReLU(),
        Linear(64, 10)
    )

    for i, layer in enumerate(model.layers):
        if isinstance(layer, QuantizedLinear):
            # Restore original weights for comparison
            original_model.layers[i].weight = dequantize_int8(
                layer.q_weight, layer.weight_scale, layer.weight_zero_point
            )
            if layer.q_bias is not None:
                original_model.layers[i].bias = dequantize_int8(
                    layer.q_bias, layer.bias_scale, layer.bias_zero_point
                )

    memory_comparison = compare_model_sizes(original_model, model)
    assert memory_comparison['compression_ratio'] > 2.0, "Insufficient compression achieved"

    print(f"âœ… Compression achieved: {memory_comparison['compression_ratio']:.1f}Ã—")
    print(f"âœ… Accuracy preserved: {relative_error:.1%} relative error")
    print(f"âœ… Memory saved: {memory_comparison['memory_saved_mb']:.1f}MB")

    # Test edge cases
    print("ğŸ”¬ Testing edge cases...")

    # Test constant tensor quantization
    constant_tensor = Tensor([[1.0, 1.0], [1.0, 1.0]])
    q_const, scale_const, zp_const = quantize_int8(constant_tensor)
    assert scale_const == 1.0, "Constant tensor quantization failed"

    # Test zero tensor
    zero_tensor = Tensor([[0.0, 0.0], [0.0, 0.0]])
    q_zero, scale_zero, zp_zero = quantize_int8(zero_tensor)
    restored_zero = dequantize_int8(q_zero, scale_zero, zp_zero)
    assert np.allclose(restored_zero.data, 0.0, atol=1e-6), "Zero tensor restoration failed"

    print("âœ… Edge cases handled correctly!")

    print("\n" + "=" * 50)
    print("ğŸ‰ ALL TESTS PASSED! Module ready for export.")
    print("ğŸ“ˆ Quantization system provides:")
    print(f"   â€¢ {memory_comparison['compression_ratio']:.1f}Ã— memory reduction")
    print(f"   â€¢ <{relative_error:.1%} accuracy loss")
    print(f"   â€¢ Production-ready INT8 quantization")
    print("Run: tito module complete 17")

# Call the comprehensive test
test_module()

# %%
if __name__ == "__main__":
    print("ğŸš€ Running Quantization module...")
    test_module()
    print("âœ… Module validation complete!")

# %% [markdown]
"""
## ğŸ Consolidated Quantization Classes for Export

Now that we've implemented all quantization components, let's create consolidated classes
for export to the tinytorch package. This allows milestones to use the complete quantization system.
"""

# %% nbgrader={"grade": false, "grade_id": "quantization_export", "solution": false}
#| export
class QuantizationComplete:
    """
    Complete quantization system for milestone use.
    
    Provides INT8 quantization with calibration for 4Ã— memory reduction.
    """
    
    @staticmethod
    def quantize_tensor(tensor: Tensor) -> Tuple[Tensor, float, int]:
        """Quantize FP32 tensor to INT8."""
        data = tensor.data
        min_val = float(np.min(data))
        max_val = float(np.max(data))
        
        if abs(max_val - min_val) < 1e-8:
            return Tensor(np.zeros_like(data, dtype=np.int8)), 1.0, 0
        
        scale = (max_val - min_val) / 255.0
        zero_point = int(np.round(-128 - min_val / scale))
        zero_point = int(np.clip(zero_point, -128, 127))
        
        quantized_data = np.round(data / scale + zero_point)
        quantized_data = np.clip(quantized_data, -128, 127).astype(np.int8)
        
        return Tensor(quantized_data), scale, zero_point
    
    @staticmethod
    def dequantize_tensor(q_tensor: Tensor, scale: float, zero_point: int) -> Tensor:
        """Dequantize INT8 tensor back to FP32."""
        dequantized_data = (q_tensor.data.astype(np.float32) - zero_point) * scale
        return Tensor(dequantized_data)
    
    @staticmethod
    def quantize_model(model, calibration_data: Optional[List[Tensor]] = None) -> Dict[str, any]:
        """
        Quantize all Linear layers in a model.
        
        Returns dictionary with quantization info and memory savings.
        """
        quantized_layers = {}
        original_size = 0
        quantized_size = 0
        
        # Iterate through model parameters
        if hasattr(model, 'parameters'):
            for i, param in enumerate(model.parameters()):
                param_size = param.data.nbytes
                original_size += param_size
                
                # Quantize parameter
                q_param, scale, zp = QuantizationComplete.quantize_tensor(param)
                quantized_size += q_param.data.nbytes
                
                quantized_layers[f'param_{i}'] = {
                    'quantized': q_param,
                    'scale': scale,
                    'zero_point': zp,
                    'original_shape': param.data.shape
                }
        
        return {
            'quantized_layers': quantized_layers,
            'original_size_mb': original_size / (1024 * 1024),
            'quantized_size_mb': quantized_size / (1024 * 1024),
            'compression_ratio': original_size / quantized_size if quantized_size > 0 else 1.0
        }
    
    @staticmethod
    def compare_models(original_model, quantized_info: Dict) -> Dict[str, float]:
        """Compare memory usage between original and quantized models."""
        return {
            'original_mb': quantized_info['original_size_mb'],
            'quantized_mb': quantized_info['quantized_size_mb'],
            'compression_ratio': quantized_info['compression_ratio'],
            'memory_saved_mb': quantized_info['original_size_mb'] - quantized_info['quantized_size_mb']
        }

# Convenience functions for backward compatibility
def quantize_int8(tensor: Tensor) -> Tuple[Tensor, float, int]:
    """Quantize FP32 tensor to INT8."""
    return QuantizationComplete.quantize_tensor(tensor)

def dequantize_int8(q_tensor: Tensor, scale: float, zero_point: int) -> Tensor:
    """Dequantize INT8 tensor back to FP32."""
    return QuantizationComplete.dequantize_tensor(q_tensor, scale, zero_point)

def quantize_model(model, calibration_data: Optional[List[Tensor]] = None) -> Dict[str, any]:
    """Quantize entire model to INT8."""
    return QuantizationComplete.quantize_model(model, calibration_data)

# %% [markdown]
"""
## ğŸ¤” ML Systems Thinking: Quantization in Production

### Question 1: Memory Architecture Impact
You implemented INT8 quantization that reduces each parameter from 4 bytes to 1 byte.
For a model with 100M parameters:
- Original memory usage: _____ GB
- Quantized memory usage: _____ GB
- Memory bandwidth reduction when loading from disk: _____ Ã—

### Question 2: Quantization Error Analysis
Your quantization maps a continuous range to 256 discrete values (INT8).
For weights uniformly distributed in [-0.1, 0.1]:
- Quantization scale: _____
- Maximum quantization error: _____
- Signal-to-noise ratio approximately: _____ dB

### Question 3: Hardware Efficiency
Modern processors have specialized INT8 instructions (like AVX-512 VNNI).
Compared to FP32 operations:
- How many INT8 operations fit in one SIMD instruction vs FP32? _____ Ã— more
- Why might actual speedup be less than this theoretical maximum? _____
- What determines whether quantization improves or hurts performance? _____

### Question 4: Calibration Strategy Trade-offs
Your calibration process finds optimal scales using sample data.
- Too little calibration data: Risk of _____
- Too much calibration data: Cost of _____
- Per-channel vs per-tensor quantization trades _____ for _____

### Question 5: Production Deployment
In mobile/edge deployment scenarios:
- When is 4Ã— memory reduction worth <1% accuracy loss? _____
- Why might you keep certain layers in FP32? _____
- How does quantization affect battery life? _____
"""

# %% [markdown]
"""
## ğŸ¯ MODULE SUMMARY: Quantization

Congratulations! You've built a complete INT8 quantization system that can reduce model size by 4Ã— with minimal accuracy loss!

### Key Accomplishments
- **Built INT8 quantization** with proper scaling and zero-point calculation
- **Implemented QuantizedLinear** layer with calibration support
- **Created model-level quantization** for complete neural networks
- **Analyzed quantization trade-offs** across different distributions and strategies
- **Measured real memory savings** and performance improvements
- All tests pass âœ… (validated by `test_module()`)

### Real-World Impact
Your quantization implementation achieves:
- **4Ã— memory reduction** (FP32 â†’ INT8)
- **2-4Ã— inference speedup** (hardware dependent)
- **<1% accuracy loss** with proper calibration
- **Production deployment readiness** for mobile/edge applications

### What You've Mastered
- **Quantization mathematics** - scale and zero-point calculations
- **Calibration techniques** - optimizing quantization parameters
- **Error analysis** - understanding and minimizing quantization noise
- **Systems optimization** - memory vs accuracy trade-offs

### Ready for Next Steps
Your quantization system enables efficient model deployment on resource-constrained devices.
Export with: `tito module complete 17`

**Next**: Module 18 will add model compression through pruning - removing unnecessary weights entirely!

---

**ğŸ† Achievement Unlocked**: You can now deploy 4Ã— smaller models with production-quality quantization! This is a critical skill for mobile AI, edge computing, and efficient inference systems.
"""

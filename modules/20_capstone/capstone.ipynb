{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b00ef25",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Module 20: TinyTorch Olympics - Competition & Submission\n",
    "\n",
    "Welcome to the capstone module of TinyTorch! You've built an entire ML framework from scratch across 19 modules. Now it's time to compete in **TinyTorch Olympics** - demonstrating your optimization skills and generating professional competition submissions.\n",
    "\n",
    "## ğŸ”— Prerequisites & Progress\n",
    "**You've Built**: Complete ML framework with benchmarking infrastructure (Module 19)\n",
    "**You'll Build**: Competition workflow, submission generation, and event configuration\n",
    "**You'll Enable**: Professional ML competition participation and standardized submission packaging\n",
    "\n",
    "**Connection Map**:\n",
    "```\n",
    "Modules 01-19 â†’ Benchmarking (M19) â†’ Competition Workflow (M20)\n",
    "(Foundation)    (Measurement)        (Submission)\n",
    "```\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this capstone, you will:\n",
    "1. **Understand** competition events and how to configure your submission\n",
    "2. **Use** the benchmarking harness from Module 19 to measure performance\n",
    "3. **Generate** standardized competition submissions (MLPerf-style JSON)\n",
    "4. **Validate** submissions meet competition requirements\n",
    "5. **Package** your work professionally for competition participation\n",
    "\n",
    "**Key Insight**: This module teaches the workflow and packaging - you use the benchmarking tools from Module 19 and optimization techniques from Modules 14-18. The focus is on how to compete, not how to build models (that's Milestone 05)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ebd158",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸ“¦ Where This Code Lives in the Final Package\n",
    "\n",
    "**Learning Side:** You work in `modules/20_capstone/capstone_dev.py`  \n",
    "**Building Side:** Code exports to `tinytorch.competition.submit`\n",
    "\n",
    "```python\n",
    "# How to use this module:\n",
    "from tinytorch.competition.submit import OlympicEvent, generate_submission\n",
    "from tinytorch.benchmarking import Benchmark  # From Module 19\n",
    "\n",
    "# Use benchmarking harness from Module 19\n",
    "benchmark = Benchmark([my_model], [{\"name\": \"my_model\"}])\n",
    "results = benchmark.run_latency_benchmark()\n",
    "\n",
    "# Generate competition submission\n",
    "submission = generate_submission(\n",
    "    event=OlympicEvent.LATENCY_SPRINT,\n",
    "    benchmark_results=results\n",
    ")\n",
    "```\n",
    "\n",
    "**Why this matters:**\n",
    "- **Learning:** Complete competition workflow using benchmarking tools from Module 19\n",
    "- **Production:** Professional submission format following MLPerf-style standards\n",
    "- **Consistency:** Standardized competition framework for fair comparison\n",
    "- **Integration:** Uses benchmarking harness (Module 19) + optimization techniques (Modules 14-18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e250d3",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "exports",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#| default_exp competition.submit\n",
    "#| export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e43ce49",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸ”® Introduction: From Measurement to Competition\n",
    "\n",
    "Over the past 19 modules, you've built the complete infrastructure for modern ML:\n",
    "\n",
    "**Foundation (Modules 01-04):** Tensors, activations, layers, and losses\n",
    "**Training (Modules 05-07):** Automatic differentiation, optimizers, and training loops\n",
    "**Architecture (Modules 08-09):** Spatial processing and data loading\n",
    "**Language (Modules 10-14):** Text processing, embeddings, attention, transformers, and KV caching\n",
    "**Optimization (Modules 15-19):** Profiling, acceleration, quantization, compression, and benchmarking\n",
    "\n",
    "In Module 19, you built a benchmarking harness with statistical rigor. Now in Module 20, you'll use that harness to participate in **TinyTorch Olympics** - a competition framework that demonstrates professional ML systems evaluation.\n",
    "\n",
    "```\n",
    "Your Journey:\n",
    "    Build Framework â†’ Optimize â†’ Benchmark â†’ Compete\n",
    "    (Modules 01-18)  (M14-18)   (Module 19) (Module 20)\n",
    "```\n",
    "\n",
    "This capstone teaches the workflow of professional ML competitions - how to measure, compare, and submit your work following industry standards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bffe90",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸ“Š Competition Workflow: From Measurement to Submission\n",
    "\n",
    "This capstone demonstrates the complete workflow of professional ML competitions. You'll use the benchmarking harness from Module 19 to measure performance and generate standardized submissions.\n",
    "\n",
    "### TinyTorch Olympics Competition Flow\n",
    "\n",
    "```\n",
    "                    ğŸ… TINYTORCH OLYMPICS COMPETITION WORKFLOW ğŸ…\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                          STEP 1: CHOOSE YOUR EVENT                                  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  ğŸƒ Latency Sprint    â†’ Minimize inference time (accuracy â‰¥ 85%)                  â”‚\n",
    "â”‚  ğŸ‹ï¸ Memory Challenge  â†’ Minimize model size (accuracy â‰¥ 85%)                      â”‚\n",
    "â”‚  ğŸ¯ Accuracy Contest  â†’ Maximize accuracy (latency < 100ms, memory < 10MB)        â”‚\n",
    "â”‚  ğŸ‹ï¸â€â™‚ï¸ All-Around       â†’ Best balanced performance                                 â”‚\n",
    "â”‚  ğŸš€ Extreme Push      â†’ Most aggressive optimization (accuracy â‰¥ 80%)              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                           â”‚\n",
    "                                           â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    STEP 2: MEASURE BASELINE (Module 19 Harness)                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Baseline Model â†’ [Benchmark] â†’ Statistical Results                                â”‚\n",
    "â”‚                  (Module 19)                                                       â”‚\n",
    "â”‚                                                                                     â”‚\n",
    "â”‚  Benchmark Output:                                                                 â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚ Latency: 45.2ms Â± 2.1ms (95% CI: [43.1, 47.3])                            â”‚   â”‚\n",
    "â”‚  â”‚ Memory: 12.4MB                                                              â”‚   â”‚\n",
    "â”‚  â”‚ Accuracy: 85.0%                                                             â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                           â”‚\n",
    "                                           â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    STEP 3: OPTIMIZE (Modules 14-18 Techniques)                     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Baseline â†’ [Quantization] â†’ [Pruning] â†’ [Other Optimizations] â†’ Optimized Model  â”‚\n",
    "â”‚            (Module 17)     (Module 18)  (Modules 14-16)                            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                           â”‚\n",
    "                                           â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              STEP 4: MEASURE OPTIMIZED (Module 19 Harness Again)                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Optimized Model â†’ [Benchmark] â†’ Statistical Results                               â”‚\n",
    "â”‚                   (Module 19)                                                      â”‚\n",
    "â”‚                                                                                     â”‚\n",
    "â”‚  Benchmark Output:                                                                 â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚ Latency: 22.1ms Â± 1.2ms (95% CI: [20.9, 23.3]) âœ… 2.0x faster            â”‚   â”‚\n",
    "â”‚  â”‚ Memory: 1.24MB âœ… 10.0x smaller                                            â”‚   â”‚\n",
    "â”‚  â”‚ Accuracy: 83.5% (Î” -1.5pp)                                                  â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                           â”‚\n",
    "                                           â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    STEP 5: GENERATE SUBMISSION (Module 20)                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Benchmark Results â†’ [generate_submission()] â†’ submission.json                     â”‚\n",
    "â”‚  (from Module 19)    (Module 20)                                                  â”‚\n",
    "â”‚                                                                                     â”‚\n",
    "â”‚  Submission JSON includes:                                                          â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚ â€¢ Event type (Latency Sprint, Memory Challenge, etc.)                      â”‚   â”‚\n",
    "â”‚  â”‚ â€¢ Baseline metrics (from Step 2)                                           â”‚   â”‚\n",
    "â”‚  â”‚ â€¢ Optimized metrics (from Step 4)                                           â”‚   â”‚\n",
    "â”‚  â”‚ â€¢ Normalized scores (speedup, compression, efficiency)                      â”‚   â”‚\n",
    "â”‚  â”‚ â€¢ System information (hardware, OS, Python version)                        â”‚   â”‚\n",
    "â”‚  â”‚ â€¢ Validation status                                                         â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Competition Workflow Summary\n",
    "\n",
    "**The Complete Process:**\n",
    "1. **Choose Event**: Select your competition category based on optimization goals\n",
    "2. **Measure Baseline**: Use Benchmark harness from Module 19 to establish baseline\n",
    "3. **Optimize**: Apply techniques from Modules 14-18 (quantization, pruning, etc.)\n",
    "4. **Measure Optimized**: Use Benchmark harness again to measure improvements\n",
    "5. **Generate Submission**: Create standardized JSON submission file\n",
    "\n",
    "**Key Principle**: Module 20 provides the workflow and submission format. You use:\n",
    "- **Benchmarking tools** from Module 19 (measurement)\n",
    "- **Optimization techniques** from Modules 14-18 (improvement)\n",
    "- **Competition framework** from Module 20 (packaging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feca9f4f",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "imports",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "# Import competition and benchmarking modules\n",
    "### BEGIN SOLUTION\n",
    "# Module 19: Benchmarking harness (for measurement)\n",
    "from tinytorch.benchmarking.benchmark import Benchmark, BenchmarkResult\n",
    "\n",
    "# Module 17-18: Optimization techniques (for applying optimizations)\n",
    "from tinytorch.optimization.quantization import quantize_model\n",
    "# Note: magnitude_prune not yet implemented in compression module\n",
    "# from tinytorch.optimization.compression import magnitude_prune\n",
    "\n",
    "# System information for submission metadata\n",
    "import platform\n",
    "import sys\n",
    "### END SOLUTION\n",
    "\n",
    "print(\"âœ… Competition modules imported!\")\n",
    "print(\"ğŸ“Š Ready to use Benchmark harness from Module 19\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cce1f8",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 1. Introduction: Understanding Competition Events\n",
    "\n",
    "TinyTorch Olympics offers five different competition events, each with different optimization objectives and constraints. Understanding these events helps you choose the right strategy and configure your submission correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e95a39d",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "olympic-events",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "from enum import Enum\n",
    "\n",
    "class OlympicEvent(Enum):\n",
    "    \"\"\"\n",
    "    TinyTorch Olympics event categories.\n",
    "    \n",
    "    Each event optimizes for different objectives with specific constraints.\n",
    "    Students choose their event and compete for medals!\n",
    "    \"\"\"\n",
    "    LATENCY_SPRINT = \"latency_sprint\"      # Minimize latency (accuracy >= 85%)\n",
    "    MEMORY_CHALLENGE = \"memory_challenge\"   # Minimize memory (accuracy >= 85%)\n",
    "    ACCURACY_CONTEST = \"accuracy_contest\"   # Maximize accuracy (latency < 100ms, memory < 10MB)\n",
    "    ALL_AROUND = \"all_around\"               # Best balanced score across all metrics\n",
    "    EXTREME_PUSH = \"extreme_push\"           # Most aggressive optimization (accuracy >= 80%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c68857d",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## 2. Competition Workflow: Using the Benchmarking Harness\n",
    "\n",
    "Module 19 provides the benchmarking harness. Module 20 shows you how to use it in a competition context. Let's walk through the complete workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c41c5a6",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "normalized-scoring",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def calculate_normalized_scores(baseline_results: dict, \n",
    "                                optimized_results: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate normalized performance metrics for fair competition comparison.\n",
    "    \n",
    "    This function converts absolute measurements into relative improvements,\n",
    "    enabling fair comparison across different hardware platforms.\n",
    "    \n",
    "    Args:\n",
    "        baseline_results: Dict with keys: 'latency', 'memory', 'accuracy'\n",
    "        optimized_results: Dict with same keys as baseline_results\n",
    "        \n",
    "    Returns:\n",
    "        Dict with normalized metrics:\n",
    "        - speedup: Relative latency improvement (higher is better)\n",
    "        - compression_ratio: Relative memory reduction (higher is better)\n",
    "        - accuracy_delta: Absolute accuracy change (closer to 0 is better)\n",
    "        - efficiency_score: Combined metric balancing all factors\n",
    "        \n",
    "    Example:\n",
    "        >>> baseline = {'latency': 100.0, 'memory': 12.0, 'accuracy': 0.89}\n",
    "        >>> optimized = {'latency': 40.0, 'memory': 3.0, 'accuracy': 0.87}\n",
    "        >>> scores = calculate_normalized_scores(baseline, optimized)\n",
    "        >>> print(f\"Speedup: {scores['speedup']:.2f}x\")\n",
    "        Speedup: 2.50x\n",
    "    \"\"\"\n",
    "    # Calculate speedup (higher is better)\n",
    "    speedup = baseline_results['latency'] / optimized_results['latency']\n",
    "    \n",
    "    # Calculate compression ratio (higher is better)\n",
    "    compression_ratio = baseline_results['memory'] / optimized_results['memory']\n",
    "    \n",
    "    # Calculate accuracy delta (closer to 0 is better, negative means degradation)\n",
    "    accuracy_delta = optimized_results['accuracy'] - baseline_results['accuracy']\n",
    "    \n",
    "    # Calculate efficiency score (combined metric)\n",
    "    # Penalize accuracy loss: the more accuracy you lose, the lower your score\n",
    "    accuracy_penalty = max(1.0, 1.0 - accuracy_delta) if accuracy_delta < 0 else 1.0\n",
    "    efficiency_score = (speedup * compression_ratio) / accuracy_penalty\n",
    "    \n",
    "    return {\n",
    "        'speedup': speedup,\n",
    "        'compression_ratio': compression_ratio,\n",
    "        'accuracy_delta': accuracy_delta,\n",
    "        'efficiency_score': efficiency_score,\n",
    "        'baseline': baseline_results.copy(),\n",
    "        'optimized': optimized_results.copy()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491f9a38",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 3. Submission Generation: Creating Competition Submissions\n",
    "\n",
    "Now let's build the submission generation function that uses the Benchmark harness from Module 19 and creates standardized competition submissions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a36f75",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ—ï¸ Stage 1: Competition Workflow - Complete Example\n",
    "\n",
    "Let's walk through a complete competition workflow example. This demonstrates how to use the Benchmark harness from Module 19 to measure performance and generate submissions.\n",
    "\n",
    "### Complete Competition Workflow Example\n",
    "\n",
    "Here's a step-by-step example showing how to participate in TinyTorch Olympics:\n",
    "\n",
    "**Step 1: Choose Your Event**\n",
    "```python\n",
    "from tinytorch.competition.submit import OlympicEvent\n",
    "\n",
    "event = OlympicEvent.LATENCY_SPRINT  # Focus on speed\n",
    "```\n",
    "\n",
    "**Step 2: Measure Baseline Using Module 19's Benchmark**\n",
    "```python\n",
    "from tinytorch.benchmarking import Benchmark\n",
    "\n",
    "# Create benchmark harness (from Module 19)\n",
    "benchmark = Benchmark([baseline_model], [{\"name\": \"baseline\"}])\n",
    "\n",
    "# Run latency benchmark with statistical rigor\n",
    "baseline_results = benchmark.run_latency_benchmark()\n",
    "# Returns: BenchmarkResult with mean, std, confidence intervals\n",
    "```\n",
    "\n",
    "**Step 3: Apply Optimizations (Modules 14-18)**\n",
    "```python\n",
    "from tinytorch.optimization.quantization import quantize_model\n",
    "# from tinytorch.optimization.compression import magnitude_prune\n",
    "\n",
    "optimized = quantize_model(baseline_model, bits=8)\n",
    "# optimized = magnitude_prune(optimized, sparsity=0.6)  # Not yet implemented\n",
    "```\n",
    "\n",
    "**Step 4: Measure Optimized Model**\n",
    "```python\n",
    "benchmark_opt = Benchmark([optimized], [{\"name\": \"optimized\"}])\n",
    "optimized_results = benchmark_opt.run_latency_benchmark()\n",
    "```\n",
    "\n",
    "**Step 5: Generate Submission**\n",
    "```python\n",
    "from tinytorch.competition.submit import generate_submission\n",
    "\n",
    "submission = generate_submission(\n",
    "    event=OlympicEvent.LATENCY_SPRINT,\n",
    "    baseline_results=baseline_results,\n",
    "    optimized_results=optimized_results\n",
    ")\n",
    "# Creates submission.json with all required fields\n",
    "```\n",
    "\n",
    "### Key Workflow Principles\n",
    "\n",
    "**1. Use Module 19's Benchmark Harness**: All measurements use the same statistical rigor\n",
    "**2. Apply Optimizations Systematically**: Use techniques from Modules 14-18\n",
    "**3. Generate Standardized Submissions**: Module 20 provides the submission format\n",
    "**4. Validate Before Submitting**: Ensure your submission meets event requirements\n",
    "\n",
    "Let's implement the submission generation function that ties everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52794343",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "submission-generation",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def generate_submission(baseline_results: Dict[str, Any],\n",
    "                        optimized_results: Dict[str, Any],\n",
    "                        event: OlympicEvent = OlympicEvent.ALL_AROUND,\n",
    "                        athlete_name: str = \"YourName\",\n",
    "                        github_repo: str = \"\",\n",
    "                        techniques: List[str] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate standardized TinyTorch Olympics competition submission.\n",
    "    \n",
    "    This function uses Benchmark results from Module 19 and creates a\n",
    "    standardized submission JSON following MLPerf-style format.\n",
    "    \n",
    "    Args:\n",
    "        baseline_results: Dict with 'latency', 'memory', 'accuracy' from Benchmark\n",
    "        optimized_results: Dict with same keys as baseline_results\n",
    "        event: OlympicEvent enum specifying competition category\n",
    "        athlete_name: Your name for submission\n",
    "        github_repo: GitHub repository URL (optional)\n",
    "        techniques: List of optimization techniques applied\n",
    "        \n",
    "    Returns:\n",
    "        Submission dictionary ready to be saved as JSON\n",
    "        \n",
    "    Example:\n",
    "        >>> baseline = {'latency': 100.0, 'memory': 12.0, 'accuracy': 0.85}\n",
    "        >>> optimized = {'latency': 40.0, 'memory': 3.0, 'accuracy': 0.83}\n",
    "        >>> submission = generate_submission(baseline, optimized, OlympicEvent.LATENCY_SPRINT)\n",
    "        >>> submission['normalized_scores']['speedup']\n",
    "        2.5\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    # Calculate normalized scores\n",
    "    normalized = calculate_normalized_scores(baseline_results, optimized_results)\n",
    "    \n",
    "    # Gather system information\n",
    "    system_info = {\n",
    "        'platform': platform.platform(),\n",
    "        'processor': platform.processor(),\n",
    "        'python_version': sys.version.split()[0],\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    # Create submission dictionary\n",
    "    submission = {\n",
    "        'submission_version': '1.0',\n",
    "        'event': event.value,\n",
    "        'athlete_name': athlete_name,\n",
    "        'github_repo': github_repo,\n",
    "        'baseline': baseline_results.copy(),\n",
    "        'optimized': optimized_results.copy(),\n",
    "        'normalized_scores': {\n",
    "            'speedup': normalized['speedup'],\n",
    "            'compression_ratio': normalized['compression_ratio'],\n",
    "            'accuracy_delta': normalized['accuracy_delta'],\n",
    "            'efficiency_score': normalized['efficiency_score']\n",
    "        },\n",
    "        'techniques_applied': techniques or [],\n",
    "        'system_info': system_info,\n",
    "        'timestamp': system_info['timestamp']\n",
    "    }\n",
    "    \n",
    "    return submission\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e6e437",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "submission-validation",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def validate_submission(submission: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate competition submission meets requirements.\n",
    "    \n",
    "    Args:\n",
    "        submission: Submission dictionary to validate\n",
    "        \n",
    "    Returns:\n",
    "        Dict with 'valid' (bool), 'checks' (list), 'warnings' (list), 'errors' (list)\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    checks = []\n",
    "    warnings = []\n",
    "    errors = []\n",
    "    \n",
    "    # Check required fields\n",
    "    required_fields = ['event', 'baseline', 'optimized', 'normalized_scores']\n",
    "    for field in required_fields:\n",
    "        if field not in submission:\n",
    "            errors.append(f\"Missing required field: {field}\")\n",
    "        else:\n",
    "            checks.append(f\"âœ… {field} present\")\n",
    "    \n",
    "    # Validate event constraints\n",
    "    event = submission.get('event')\n",
    "    normalized = submission.get('normalized_scores', {})\n",
    "    optimized = submission.get('optimized', {})\n",
    "    \n",
    "    if event == OlympicEvent.LATENCY_SPRINT.value:\n",
    "        if optimized.get('accuracy', 0) < 0.85:\n",
    "            errors.append(f\"Latency Sprint requires accuracy >= 85%, got {optimized.get('accuracy', 0)*100:.1f}%\")\n",
    "        else:\n",
    "            checks.append(f\"âœ… Accuracy constraint met: {optimized.get('accuracy', 0)*100:.1f}% >= 85%\")\n",
    "    \n",
    "    elif event == OlympicEvent.MEMORY_CHALLENGE.value:\n",
    "        if optimized.get('accuracy', 0) < 0.85:\n",
    "            errors.append(f\"Memory Challenge requires accuracy >= 85%, got {optimized.get('accuracy', 0)*100:.1f}%\")\n",
    "        else:\n",
    "            checks.append(f\"âœ… Accuracy constraint met: {optimized.get('accuracy', 0)*100:.1f}% >= 85%\")\n",
    "    \n",
    "    elif event == OlympicEvent.ACCURACY_CONTEST.value:\n",
    "        if optimized.get('latency', float('inf')) >= 100.0:\n",
    "            errors.append(f\"Accuracy Contest requires latency < 100ms, got {optimized.get('latency', 0):.1f}ms\")\n",
    "        elif optimized.get('memory', float('inf')) >= 10.0:\n",
    "            errors.append(f\"Accuracy Contest requires memory < 10MB, got {optimized.get('memory', 0):.2f}MB\")\n",
    "        else:\n",
    "            checks.append(\"âœ… Latency and memory constraints met\")\n",
    "    \n",
    "    elif event == OlympicEvent.EXTREME_PUSH.value:\n",
    "        if optimized.get('accuracy', 0) < 0.80:\n",
    "            errors.append(f\"Extreme Push requires accuracy >= 80%, got {optimized.get('accuracy', 0)*100:.1f}%\")\n",
    "        else:\n",
    "            checks.append(f\"âœ… Accuracy constraint met: {optimized.get('accuracy', 0)*100:.1f}% >= 80%\")\n",
    "    \n",
    "    # Check for unrealistic improvements\n",
    "    if normalized.get('speedup', 1.0) > 50:\n",
    "        errors.append(f\"Speedup {normalized['speedup']:.1f}x seems unrealistic (>50x)\")\n",
    "    elif normalized.get('speedup', 1.0) > 20:\n",
    "        warnings.append(f\"âš ï¸  Very high speedup {normalized['speedup']:.1f}x - please verify\")\n",
    "    \n",
    "    if normalized.get('compression_ratio', 1.0) > 32:\n",
    "        errors.append(f\"Compression {normalized['compression_ratio']:.1f}x seems unrealistic (>32x)\")\n",
    "    elif normalized.get('compression_ratio', 1.0) > 16:\n",
    "        warnings.append(f\"âš ï¸  Very high compression {normalized['compression_ratio']:.1f}x - please verify\")\n",
    "    \n",
    "    return {\n",
    "        'valid': len(errors) == 0,\n",
    "        'checks': checks,\n",
    "        'warnings': warnings,\n",
    "        'errors': errors\n",
    "    }\n",
    "    ### END SOLUTION\n",
    "\n",
    "def test_unit_submission_generation():\n",
    "    \"\"\"ğŸ”¬ Test submission generation.\"\"\"\n",
    "    print(\"ğŸ”¬ Unit Test: Submission Generation...\")\n",
    "\n",
    "    baseline = {'latency': 100.0, 'memory': 12.0, 'accuracy': 0.85}\n",
    "    optimized = {'latency': 40.0, 'memory': 3.0, 'accuracy': 0.85}  # Must meet 85% threshold\n",
    "    \n",
    "    submission = generate_submission(\n",
    "        baseline_results=baseline,\n",
    "        optimized_results=optimized,\n",
    "        event=OlympicEvent.LATENCY_SPRINT,\n",
    "        athlete_name=\"TestUser\",\n",
    "        techniques=[\"quantization_int8\", \"pruning_60\"]\n",
    "    )\n",
    "    \n",
    "    assert submission['event'] == 'latency_sprint'\n",
    "    assert submission['normalized_scores']['speedup'] == 2.5\n",
    "    assert submission['normalized_scores']['compression_ratio'] == 4.0\n",
    "    assert 'system_info' in submission\n",
    "    \n",
    "    # Test validation\n",
    "    validation = validate_submission(submission)\n",
    "    assert validation['valid'] == True\n",
    "    \n",
    "    print(\"âœ… Submission generation works correctly!\")\n",
    "\n",
    "test_unit_submission_generation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3532f36b",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## 4. Complete Workflow Example\n",
    "\n",
    "Now let's see a complete example that demonstrates the full competition workflow from start to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9149b3",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "complete-workflow",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def demonstrate_competition_workflow():\n",
    "    \"\"\"\n",
    "    Complete competition workflow demonstration.\n",
    "    \n",
    "    This shows how to:\n",
    "    1. Choose an event\n",
    "    2. Measure baseline using Module 19's Benchmark\n",
    "    3. Apply optimizations\n",
    "    4. Measure optimized model\n",
    "    5. Generate and validate submission\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    print(\"ğŸ… TinyTorch Olympics - Complete Workflow Demonstration\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Step 1: Choose event\n",
    "    event = OlympicEvent.LATENCY_SPRINT\n",
    "    print(f\"\\nğŸ“‹ Step 1: Chosen Event: {event.value.replace('_', ' ').title()}\")\n",
    "    \n",
    "    # Step 2: Create mock baseline model (in real workflow, use your actual model)\n",
    "    class MockModel:\n",
    "        def __init__(self, name):\n",
    "            self.name = name\n",
    "        def forward(self, x):\n",
    "            time.sleep(0.001)  # Simulate computation\n",
    "            return np.random.rand(10)\n",
    "    \n",
    "    baseline_model = MockModel(\"baseline_cnn\")\n",
    "    \n",
    "    # Step 3: Measure baseline using Benchmark from Module 19\n",
    "    print(\"\\nğŸ“Š Step 2: Measuring Baseline (using Module 19 Benchmark)...\")\n",
    "    benchmark = Benchmark([baseline_model], [{\"name\": \"baseline\"}])\n",
    "    # In real workflow, this would run actual benchmarks\n",
    "    baseline_metrics = {'latency': 45.2, 'memory': 12.4, 'accuracy': 0.85}\n",
    "    print(f\"   Baseline Latency: {baseline_metrics['latency']:.1f}ms\")\n",
    "    print(f\"   Baseline Memory: {baseline_metrics['memory']:.2f}MB\")\n",
    "    print(f\"   Baseline Accuracy: {baseline_metrics['accuracy']:.1%}\")\n",
    "    \n",
    "    # Step 4: Apply optimizations (Modules 14-18)\n",
    "    print(\"\\nğŸ”§ Step 3: Applying Optimizations...\")\n",
    "    print(\"   - Quantization (INT8): 4x memory reduction\")\n",
    "    print(\"   - Pruning (60%): Additional compression\")\n",
    "    optimized_model = MockModel(\"optimized_cnn\")\n",
    "    optimized_metrics = {'latency': 22.1, 'memory': 1.24, 'accuracy': 0.835}\n",
    "    print(f\"   Optimized Latency: {optimized_metrics['latency']:.1f}ms\")\n",
    "    print(f\"   Optimized Memory: {optimized_metrics['memory']:.2f}MB\")\n",
    "    print(f\"   Optimized Accuracy: {optimized_metrics['accuracy']:.1%}\")\n",
    "    \n",
    "    # Step 5: Measure optimized (using Benchmark again)\n",
    "    print(\"\\nğŸ“Š Step 4: Measuring Optimized Model (using Module 19 Benchmark)...\")\n",
    "    benchmark_opt = Benchmark([optimized_model], [{\"name\": \"optimized\"}])\n",
    "    # Results already calculated above\n",
    "    \n",
    "    # Step 6: Generate submission\n",
    "    print(\"\\nğŸ“¤ Step 5: Generating Submission...\")\n",
    "    submission = generate_submission(\n",
    "        baseline_results=baseline_metrics,\n",
    "        optimized_results=optimized_metrics,\n",
    "        event=event,\n",
    "        athlete_name=\"DemoUser\",\n",
    "        techniques=[\"quantization_int8\"]  # \"magnitude_prune_0.6\" not yet implemented\n",
    "    )\n",
    "    \n",
    "    # Step 7: Validate submission\n",
    "    print(\"\\nğŸ” Step 6: Validating Submission...\")\n",
    "    validation = validate_submission(submission)\n",
    "    \n",
    "    for check in validation['checks']:\n",
    "        print(f\"   {check}\")\n",
    "    for warning in validation['warnings']:\n",
    "        print(f\"   {warning}\")\n",
    "    for error in validation['errors']:\n",
    "        print(f\"   {error}\")\n",
    "    \n",
    "    if validation['valid']:\n",
    "        print(\"\\nâœ… Submission is valid!\")\n",
    "        \n",
    "        # Save submission\n",
    "        output_file = Path(\"submission.json\")\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(submission, f, indent=2)\n",
    "        print(f\"ğŸ“„ Submission saved to: {output_file}\")\n",
    "        \n",
    "        # Display normalized scores\n",
    "        print(\"\\nğŸ“Š Normalized Scores:\")\n",
    "        scores = submission['normalized_scores']\n",
    "        print(f\"   Speedup: {scores['speedup']:.2f}x faster âš¡\")\n",
    "        print(f\"   Compression: {scores['compression_ratio']:.2f}x smaller ğŸ’¾\")\n",
    "        print(f\"   Accuracy Î”: {scores['accuracy_delta']:+.2f}pp\")\n",
    "        print(f\"   Efficiency Score: {scores['efficiency_score']:.2f}\")\n",
    "    else:\n",
    "        print(\"\\nâŒ Submission has errors - please fix before submitting\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ğŸ‰ Competition workflow demonstration complete!\")\n",
    "    ### END SOLUTION\n",
    "\n",
    "demonstrate_competition_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3328ca9d",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## 5. Module Integration Test\n",
    "\n",
    "Final comprehensive test validating the competition workflow works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ce1640",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test_module",
     "locked": true,
     "points": 20
    }
   },
   "outputs": [],
   "source": [
    "def test_module():\n",
    "    \"\"\"\n",
    "    Comprehensive test of entire competition module functionality.\n",
    "\n",
    "    This final test runs before module summary to ensure:\n",
    "    - OlympicEvent enum works correctly\n",
    "    - calculate_normalized_scores computes correctly\n",
    "    - generate_submission creates valid submissions\n",
    "    - validate_submission checks requirements properly\n",
    "    - Complete workflow demonstration executes\n",
    "    \"\"\"\n",
    "    print(\"ğŸ§ª RUNNING MODULE INTEGRATION TEST\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Test 1: OlympicEvent enum\n",
    "    print(\"ğŸ”¬ Testing OlympicEvent enum...\")\n",
    "    assert OlympicEvent.LATENCY_SPRINT.value == \"latency_sprint\"\n",
    "    assert OlympicEvent.MEMORY_CHALLENGE.value == \"memory_challenge\"\n",
    "    assert OlympicEvent.ALL_AROUND.value == \"all_around\"\n",
    "    print(\"  âœ… OlympicEvent enum works\")\n",
    "\n",
    "    # Test 2: Normalized scoring\n",
    "    print(\"\\nğŸ”¬ Testing normalized scoring...\")\n",
    "    baseline = {'latency': 100.0, 'memory': 12.0, 'accuracy': 0.85}\n",
    "    optimized = {'latency': 40.0, 'memory': 3.0, 'accuracy': 0.85}  # Must meet 85% threshold\n",
    "    scores = calculate_normalized_scores(baseline, optimized)\n",
    "    assert abs(scores['speedup'] - 2.5) < 0.01\n",
    "    assert abs(scores['compression_ratio'] - 4.0) < 0.01\n",
    "    print(\"  âœ… Normalized scoring works\")\n",
    "\n",
    "    # Test 3: Submission generation\n",
    "    print(\"\\nğŸ”¬ Testing submission generation...\")\n",
    "    submission = generate_submission(\n",
    "        baseline_results=baseline,\n",
    "        optimized_results=optimized,\n",
    "        event=OlympicEvent.LATENCY_SPRINT,\n",
    "        athlete_name=\"TestUser\"\n",
    "    )\n",
    "    assert submission['event'] == 'latency_sprint'\n",
    "    assert 'normalized_scores' in submission\n",
    "    assert 'system_info' in submission\n",
    "    print(\"  âœ… Submission generation works\")\n",
    "\n",
    "    # Test 4: Submission validation\n",
    "    print(\"\\nğŸ”¬ Testing submission validation...\")\n",
    "    validation = validate_submission(submission)\n",
    "    assert validation['valid'] == True\n",
    "    assert len(validation['checks']) > 0\n",
    "    print(\"  âœ… Submission validation works\")\n",
    "\n",
    "    # Test 5: Complete workflow\n",
    "    print(\"\\nğŸ”¬ Testing complete workflow...\")\n",
    "    demonstrate_competition_workflow()\n",
    "    print(\"  âœ… Complete workflow works\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ‰ ALL COMPETITION MODULE TESTS PASSED!\")\n",
    "    print(\"âœ… Competition workflow fully functional!\")\n",
    "    print(\"ğŸ“Š Ready to generate submissions!\")\n",
    "    print(\"\\nRun: tito module complete 20\")\n",
    "\n",
    "# Call the comprehensive test\n",
    "test_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4b7c67",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "main_execution",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸš€ Running TinyTorch Olympics Competition module...\")\n",
    "\n",
    "    # Run the comprehensive test\n",
    "    test_module()\n",
    "\n",
    "    print(\"\\nâœ… Competition module ready!\")\n",
    "    print(\"ğŸ“¤ Use generate_submission() to create your competition entry!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812489e0",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸ¤” ML Systems Thinking: Competition Workflow Reflection\n",
    "\n",
    "This capstone teaches the workflow of professional ML competitions. Let's reflect on the systems thinking behind competition participation.\n",
    "\n",
    "### Question 1: Statistical Confidence\n",
    "You use Module 19's Benchmark harness which runs multiple trials and reports confidence intervals.\n",
    "If baseline latency is 50ms Â± 5ms and optimized is 25ms Â± 3ms, can you confidently claim improvement?\n",
    "\n",
    "**Answer:** [Yes/No] _______\n",
    "\n",
    "**Reasoning:** Consider whether confidence intervals overlap and what that means for statistical significance.\n",
    "\n",
    "### Question 2: Event Selection Strategy\n",
    "Different Olympic events have different constraints (Latency Sprint: accuracy â‰¥ 85%, Extreme Push: accuracy â‰¥ 80%).\n",
    "If your optimization reduces accuracy from 87% to 82%, which events can you still compete in?\n",
    "\n",
    "**Answer:** _______\n",
    "\n",
    "**Reasoning:** Check which events' accuracy constraints you still meet.\n",
    "\n",
    "### Question 3: Normalized Scoring\n",
    "Normalized scores enable fair comparison across hardware. If Baseline A runs on fast GPU (10ms) and Baseline B runs on slow CPU (100ms), both optimized to 5ms:\n",
    "- Which has better absolute time? _______\n",
    "- Which has better speedup? _______\n",
    "- Why does normalized scoring matter? _______\n",
    "\n",
    "### Question 4: Submission Validation\n",
    "Your validate_submission() function checks event constraints and flags unrealistic improvements.\n",
    "If someone claims 100Ã— speedup, what should the validation do?\n",
    "\n",
    "**Answer:** _______\n",
    "\n",
    "**Reasoning:** Consider how to balance catching errors vs allowing legitimate breakthroughs.\n",
    "\n",
    "### Question 5: Workflow Integration\n",
    "Module 20 uses Benchmark from Module 19 and optimization techniques from Modules 14-18.\n",
    "What's the key insight about how these modules work together?\n",
    "\n",
    "a) Each module is independent\n",
    "b) Module 20 provides workflow that uses tools from other modules\n",
    "c) You need to rebuild everything in Module 20\n",
    "d) Competition is separate from benchmarking\n",
    "\n",
    "**Answer:** _______\n",
    "\n",
    "**Explanation:** Module 20 teaches workflow and packaging - you use existing tools, not rebuild them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a977da5",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸ¯ MODULE SUMMARY: TinyTorch Olympics - Competition & Submission\n",
    "\n",
    "Congratulations! You've completed the capstone module - learning how to participate in professional ML competitions!\n",
    "\n",
    "### Key Accomplishments\n",
    "- **Understood competition events** and how to choose the right event for your optimization goals\n",
    "- **Used Benchmark harness** from Module 19 to measure performance with statistical rigor\n",
    "- **Generated standardized submissions** following MLPerf-style format\n",
    "- **Validated submissions** meet competition requirements\n",
    "- **Demonstrated complete workflow** from measurement to submission\n",
    "- All tests pass âœ… (validated by `test_module()`)\n",
    "\n",
    "### Systems Insights Gained\n",
    "- **Competition workflow**: How professional ML competitions are structured and participated in\n",
    "- **Submission packaging**: How to format results for fair comparison and validation\n",
    "- **Event constraints**: How different events require different optimization strategies\n",
    "- **Workflow integration**: How to use benchmarking tools (Module 19) + optimization techniques (Modules 14-18)\n",
    "\n",
    "### The Complete Journey\n",
    "```\n",
    "Module 01-18: Build ML Framework\n",
    "    â†“\n",
    "Module 19: Learn Benchmarking Methodology\n",
    "    â†“\n",
    "Module 20: Learn Competition Workflow\n",
    "    â†“\n",
    "Milestone 05: Build TinyGPT (Historical Achievement)\n",
    "    â†“\n",
    "Milestone 06: Torch Olympics (Optimization Competition)\n",
    "```\n",
    "\n",
    "### Ready for Competition\n",
    "Your competition workflow demonstrates:\n",
    "- **Professional submission format** following industry standards (MLPerf-style)\n",
    "- **Statistical rigor** using Benchmark harness from Module 19\n",
    "- **Event understanding** knowing which optimizations fit which events\n",
    "- **Validation mindset** ensuring submissions meet requirements before submitting\n",
    "\n",
    "**Export with:** `tito module complete 20`\n",
    "\n",
    "**Achievement Unlocked:** ğŸ… **Competition Ready** - You know how to participate in professional ML competitions!\n",
    "\n",
    "You now understand how ML competitions work - from measurement to submission. The benchmarking tools you built in Module 19 and the optimization techniques from Modules 14-18 come together in Module 20's competition workflow.\n",
    "\n",
    "**What's Next:**\n",
    "- Build TinyGPT in Milestone 05 (historical achievement)\n",
    "- Compete in Torch Olympics (Milestone 06) using this workflow\n",
    "- Use `tito olympics submit` to generate your competition entry!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../modules/19_benchmarking/benchmarking.ipynb.

# %% auto 0
__all__ = ['BenchmarkResult', 'test_unit_benchmark_result', 'precise_timer', 'test_unit_precise_timer', 'Benchmark',
           'test_unit_benchmark', 'BenchmarkSuite', 'test_unit_benchmark_suite']

# %% ../../modules/19_benchmarking/benchmarking.ipynb 0
#| default_exp benchmarking.benchmark
#| export

# %% ../../modules/19_benchmarking/benchmarking.ipynb 5
import numpy as np
import time
import statistics
import os
import tracemalloc
from typing import Dict, List, Tuple, Any, Optional, Callable, Union
from dataclasses import dataclass, field
from pathlib import Path
import platform
from contextlib import contextmanager

# Optional dependency for visualization only
try:
    import matplotlib.pyplot as plt
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False
    # Create minimal fallback for when matplotlib is not available
    class plt:
        @staticmethod
        def subplots(*args, **kwargs):
            return None, None
        @staticmethod
        def figure(*args, **kwargs):
            return None
        @staticmethod
        def scatter(*args, **kwargs):
            pass
        @staticmethod
        def annotate(*args, **kwargs):
            pass
        @staticmethod
        def xlabel(*args, **kwargs):
            pass
        @staticmethod
        def ylabel(*args, **kwargs):
            pass
        @staticmethod
        def title(*args, **kwargs):
            pass
        @staticmethod
        def grid(*args, **kwargs):
            pass
        @staticmethod
        def tight_layout(*args, **kwargs):
            pass
        @staticmethod
        def savefig(*args, **kwargs):
            pass
        @staticmethod
        def show(*args, **kwargs):
            pass

# %% ../../modules/19_benchmarking/benchmarking.ipynb 6
# Import Profiler from Module 14 for measurement reuse
from ..profiling.profiler import Profiler

# %% ../../modules/19_benchmarking/benchmarking.ipynb 9
@dataclass
class BenchmarkResult:
    """
    Container for benchmark measurements with statistical analysis.

    TODO: Implement a robust result container that stores measurements and metadata

    APPROACH:
    1. Store raw measurements and computed statistics
    2. Include metadata about test conditions
    3. Provide methods for statistical analysis
    4. Support serialization for result persistence

    EXAMPLE:
    >>> result = BenchmarkResult("model_accuracy", [0.95, 0.94, 0.96])
    >>> print(f"Mean: {result.mean:.3f} Â± {result.std:.3f}")
    Mean: 0.950 Â± 0.010

    HINTS:
    - Use statistics module for robust mean/std calculations
    - Store both raw data and summary statistics
    - Include confidence intervals for professional reporting
    """
    ### BEGIN SOLUTION
    metric_name: str
    values: List[float]
    metadata: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        """Compute statistics after initialization."""
        if not self.values:
            raise ValueError("BenchmarkResult requires at least one measurement")

        self.mean = statistics.mean(self.values)
        self.std = statistics.stdev(self.values) if len(self.values) > 1 else 0.0
        self.median = statistics.median(self.values)
        self.min_val = min(self.values)
        self.max_val = max(self.values)
        self.count = len(self.values)

        # 95% confidence interval for the mean
        if len(self.values) > 1:
            t_score = 1.96  # Approximate for large samples
            margin_error = t_score * (self.std / np.sqrt(self.count))
            self.ci_lower = self.mean - margin_error
            self.ci_upper = self.mean + margin_error
        else:
            self.ci_lower = self.ci_upper = self.mean

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            'metric_name': self.metric_name,
            'values': self.values,
            'mean': self.mean,
            'std': self.std,
            'median': self.median,
            'min': self.min_val,
            'max': self.max_val,
            'count': self.count,
            'ci_lower': self.ci_lower,
            'ci_upper': self.ci_upper,
            'metadata': self.metadata
        }

    def __str__(self) -> str:
        return f"{self.metric_name}: {self.mean:.4f} Â± {self.std:.4f} (n={self.count})"
    ### END SOLUTION

def test_unit_benchmark_result():
    """ðŸ”¬ Test BenchmarkResult statistical calculations."""
    print("ðŸ”¬ Unit Test: BenchmarkResult...")

    # Test basic statistics
    values = [1.0, 2.0, 3.0, 4.0, 5.0]
    result = BenchmarkResult("test_metric", values)

    assert result.mean == 3.0
    assert abs(result.std - statistics.stdev(values)) < 1e-10
    assert result.median == 3.0
    assert result.min_val == 1.0
    assert result.max_val == 5.0
    assert result.count == 5

    # Test confidence intervals
    assert result.ci_lower < result.mean < result.ci_upper

    # Test serialization
    result_dict = result.to_dict()
    assert result_dict['metric_name'] == "test_metric"
    assert result_dict['mean'] == 3.0

    print("âœ… BenchmarkResult works correctly!")

test_unit_benchmark_result()

# %% ../../modules/19_benchmarking/benchmarking.ipynb 11
@contextmanager
def precise_timer():
    """
    High-precision timing context manager for benchmarking.

    TODO: Implement a context manager that provides accurate timing measurements

    APPROACH:
    1. Use time.perf_counter() for high precision
    2. Handle potential interruptions and system noise
    3. Return elapsed time when context exits
    4. Provide warmup capability for JIT compilation

    EXAMPLE:
    >>> with precise_timer() as timer:
    ...     time.sleep(0.1)  # Some operation
    >>> print(f"Elapsed: {timer.elapsed:.4f}s")
    Elapsed: 0.1001s

    HINTS:
    - perf_counter() is monotonic and high-resolution
    - Store start time in __enter__, compute elapsed in __exit__
    - Handle any exceptions gracefully
    """
    ### BEGIN SOLUTION
    class Timer:
        def __init__(self):
            self.elapsed = 0.0
            self.start_time = None

    timer = Timer()
    timer.start_time = time.perf_counter()
    try:
        yield timer
    finally:
        if timer.start_time is not None:
            timer.elapsed = time.perf_counter() - timer.start_time
    ### END SOLUTION

def test_unit_precise_timer():
    """ðŸ”¬ Test precise_timer context manager."""
    print("ðŸ”¬ Unit Test: precise_timer...")

    # Test basic timing
    with precise_timer() as timer:
        time.sleep(0.01)  # 10ms sleep

    # Should be close to 0.01 seconds (allow some variance)
    assert 0.005 < timer.elapsed < 0.05, f"Expected ~0.01s, got {timer.elapsed}s"

    # Test multiple uses
    times = []
    for _ in range(3):
        with precise_timer() as timer:
            time.sleep(0.001)  # 1ms sleep
        times.append(timer.elapsed)

    # All times should be reasonably close
    assert all(0.0005 < t < 0.01 for t in times)

    print("âœ… precise_timer works correctly!")

test_unit_precise_timer()

# %% ../../modules/19_benchmarking/benchmarking.ipynb 13
class Benchmark:
    """
    Professional benchmarking system for ML models and operations.

    TODO: Implement a comprehensive benchmark runner with statistical rigor

    APPROACH:
    1. Support multiple models, datasets, and metrics
    2. Run repeated measurements with proper warmup
    3. Control for system variance and compute confidence intervals
    4. Generate structured results for analysis

    EXAMPLE:
    >>> benchmark = Benchmark(models=[model1, model2], datasets=[test_data])
    >>> results = benchmark.run_accuracy_benchmark()
    >>> benchmark.plot_results(results)

    HINTS:
    - Use warmup runs to stabilize performance
    - Collect multiple samples for statistical significance
    - Store metadata about system conditions
    - Provide different benchmark types (accuracy, latency, memory)
    """
    ### BEGIN SOLUTION
    def __init__(self, models: List[Any], datasets: List[Any],
                 warmup_runs: int = 5, measurement_runs: int = 10):
        """Initialize benchmark with models and datasets."""
        self.models = models
        self.datasets = datasets
        self.warmup_runs = warmup_runs
        self.measurement_runs = measurement_runs
        self.results = {}
        
        # Use Profiler from Module 15 for measurements
        self.profiler = Profiler()

        # System information for metadata (using Python standard library)
        self.system_info = {
            'platform': platform.platform(),
            'processor': platform.processor(),
            'python_version': platform.python_version(),
            'cpu_count': os.cpu_count() or 1,  # os.cpu_count() can return None
        }
        # Note: System total memory not available via standard library
        # Process memory measurement uses tracemalloc (via Profiler)

    def run_latency_benchmark(self, input_shape: Tuple[int, ...] = (1, 28, 28)) -> Dict[str, BenchmarkResult]:
        """Benchmark model inference latency using Profiler."""
        results = {}

        for i, model in enumerate(self.models):
            model_name = getattr(model, 'name', f'model_{i}')
            
            # Create input tensor for profiling
            try:
                from tinytorch.core.tensor import Tensor
                input_tensor = Tensor(np.random.randn(*input_shape).astype(np.float32))
            except:
                # Fallback for simple models
                input_tensor = np.random.randn(*input_shape).astype(np.float32)

            # Use Profiler to measure latency with proper warmup and iterations
            try:
                latency_ms = self.profiler.measure_latency(
                    model, 
                    input_tensor,
                    warmup=self.warmup_runs,
                    iterations=self.measurement_runs
                )
                
                # Profiler returns single median value
                # For BenchmarkResult, we need multiple measurements
                # Run additional measurements for statistical analysis
                latencies = []
                for _ in range(self.measurement_runs):
                    single_latency = self.profiler.measure_latency(
                        model, input_tensor, warmup=0, iterations=1
                    )
                    latencies.append(single_latency)
                
            except:
                # Fallback: use precise_timer for models that don't support profiler
                latencies = []
                for _ in range(self.measurement_runs):
                    with precise_timer() as timer:
                        try:
                            if hasattr(model, 'forward'):
                                model.forward(input_tensor)
                            elif hasattr(model, 'predict'):
                                model.predict(input_tensor)
                            elif callable(model):
                                model(input_tensor)
                            else:
                                time.sleep(0.001)
                        except:
                            time.sleep(0.001 + np.random.normal(0, 0.0001))
                    latencies.append(timer.elapsed * 1000)

            results[model_name] = BenchmarkResult(
                f"{model_name}_latency_ms",
                latencies,
                metadata={'input_shape': input_shape, **self.system_info}
            )

        return results

    def run_accuracy_benchmark(self) -> Dict[str, BenchmarkResult]:
        """Benchmark model accuracy across datasets."""
        results = {}

        for i, model in enumerate(self.models):
            model_name = getattr(model, 'name', f'model_{i}')
            accuracies = []

            for dataset in self.datasets:
                # Simulate accuracy measurement
                # In practice, this would evaluate the model on the dataset
                try:
                    if hasattr(model, 'evaluate'):
                        accuracy = model.evaluate(dataset)
                    else:
                        # Simulate accuracy for demonstration
                        base_accuracy = 0.85 + i * 0.05  # Different models have different base accuracies
                        accuracy = base_accuracy + np.random.normal(0, 0.02)  # Add noise
                        accuracy = max(0.0, min(1.0, accuracy))  # Clamp to [0, 1]
                except:
                    # Fallback simulation
                    accuracy = 0.80 + np.random.normal(0, 0.05)
                    accuracy = max(0.0, min(1.0, accuracy))

                accuracies.append(accuracy)

            results[model_name] = BenchmarkResult(
                f"{model_name}_accuracy",
                accuracies,
                metadata={'num_datasets': len(self.datasets), **self.system_info}
            )

        return results

    def run_memory_benchmark(self, input_shape: Tuple[int, ...] = (1, 28, 28)) -> Dict[str, BenchmarkResult]:
        """Benchmark model memory usage using Profiler."""
        results = {}

        for i, model in enumerate(self.models):
            model_name = getattr(model, 'name', f'model_{i}')
            memory_usages = []

            for run in range(self.measurement_runs):
                try:
                    # Use Profiler to measure memory
                    memory_stats = self.profiler.measure_memory(model, input_shape)
                    # Use peak_memory_mb as the primary metric
                    memory_used = memory_stats['peak_memory_mb']
                except:
                    # Fallback: use tracemalloc (Python standard library) for memory measurement
                    tracemalloc.start()
                    baseline_memory = tracemalloc.get_traced_memory()[0] / (1024**2)  # MB

                    try:
                        dummy_input = np.random.randn(*input_shape).astype(np.float32)
                        if hasattr(model, 'forward'):
                            model.forward(dummy_input)
                        elif hasattr(model, 'predict'):
                            model.predict(dummy_input)
                        elif callable(model):
                            model(dummy_input)
                    except:
                        pass

                    peak_memory = tracemalloc.get_traced_memory()[1] / (1024**2)  # MB
                    tracemalloc.stop()
                    memory_used = max(0, peak_memory - baseline_memory)

                    # If no significant memory change detected, estimate from parameters
                    if memory_used < 1.0:
                        try:
                            param_count = self.profiler.count_parameters(model)
                            memory_used = param_count * 4 / (1024**2)  # 4 bytes per float32
                        except:
                            memory_used = 8 + np.random.normal(0, 1)  # Default estimate

                memory_usages.append(max(0, memory_used))

            results[model_name] = BenchmarkResult(
                f"{model_name}_memory_mb",
                memory_usages,
                metadata={'input_shape': input_shape, **self.system_info}
            )

        return results

    def compare_models(self, metric: str = "latency") -> List[Dict[str, Any]]:
        """
        Compare models across a specific metric.
        
        Returns a list of dictionaries, one per model, with comparison metrics.
        This keeps dependencies minimal - students can convert to DataFrame if needed.
        """
        if metric == "latency":
            results = self.run_latency_benchmark()
        elif metric == "accuracy":
            results = self.run_accuracy_benchmark()
        elif metric == "memory":
            results = self.run_memory_benchmark()
        else:
            raise ValueError(f"Unknown metric: {metric}")

        # Return structured list of dicts for easy comparison
        # (No pandas dependency - students can convert to DataFrame if needed)
        comparison_data = []
        for model_name, result in results.items():
            comparison_data.append({
                'model': model_name.replace(f'_{metric}', '').replace('_ms', '').replace('_mb', ''),
                'metric': metric,
                'mean': result.mean,
                'std': result.std,
                'ci_lower': result.ci_lower,
                'ci_upper': result.ci_upper,
                'count': result.count
            })

        return comparison_data
    ### END SOLUTION

def test_unit_benchmark():
    """ðŸ”¬ Test Benchmark class functionality."""
    print("ðŸ”¬ Unit Test: Benchmark...")

    # Create mock models for testing
    class MockModel:
        def __init__(self, name):
            self.name = name

        def forward(self, x):
            time.sleep(0.001)  # Simulate computation
            return x

    models = [MockModel("fast_model"), MockModel("slow_model")]
    datasets = [{"data": "test1"}, {"data": "test2"}]

    benchmark = Benchmark(models, datasets, warmup_runs=2, measurement_runs=3)

    # Test latency benchmark
    latency_results = benchmark.run_latency_benchmark()
    assert len(latency_results) == 2
    assert "fast_model" in latency_results
    assert all(isinstance(result, BenchmarkResult) for result in latency_results.values())

    # Test accuracy benchmark
    accuracy_results = benchmark.run_accuracy_benchmark()
    assert len(accuracy_results) == 2
    assert all(0 <= result.mean <= 1 for result in accuracy_results.values())

    # Test memory benchmark
    memory_results = benchmark.run_memory_benchmark()
    assert len(memory_results) == 2
    assert all(result.mean >= 0 for result in memory_results.values())

    # Test comparison (returns list of dicts, not DataFrame)
    comparison_data = benchmark.compare_models("latency")
    assert len(comparison_data) == 2
    assert isinstance(comparison_data, list)
    assert all(isinstance(item, dict) for item in comparison_data)
    assert "model" in comparison_data[0]
    assert "mean" in comparison_data[0]

    print("âœ… Benchmark works correctly!")

test_unit_benchmark()

# %% ../../modules/19_benchmarking/benchmarking.ipynb 15
class BenchmarkSuite:
    """
    Comprehensive benchmark suite for ML systems evaluation.

    TODO: Implement a full benchmark suite that runs multiple test categories

    APPROACH:
    1. Combine multiple benchmark types (latency, accuracy, memory, energy)
    2. Generate comprehensive reports with visualizations
    3. Support different model categories and hardware configurations
    4. Provide recommendations based on results

    EXAMPLE:
    >>> suite = BenchmarkSuite(models, datasets)
    >>> report = suite.run_full_benchmark()
    >>> suite.generate_report(report)

    HINTS:
    - Organize results by benchmark type and model
    - Create Pareto frontier analysis for trade-offs
    - Include system information and test conditions
    - Generate actionable insights and recommendations
    """
    ### BEGIN SOLUTION
    def __init__(self, models: List[Any], datasets: List[Any],
                 output_dir: str = "benchmark_results"):
        """Initialize comprehensive benchmark suite."""
        self.models = models
        self.datasets = datasets
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)

        self.benchmark = Benchmark(models, datasets)
        self.results = {}

    def run_full_benchmark(self) -> Dict[str, Dict[str, BenchmarkResult]]:
        """Run all benchmark categories."""
        print("ðŸ”¬ Running comprehensive benchmark suite...")

        # Run all benchmark types
        print("  ðŸ“Š Measuring latency...")
        self.results['latency'] = self.benchmark.run_latency_benchmark()

        print("  ðŸŽ¯ Measuring accuracy...")
        self.results['accuracy'] = self.benchmark.run_accuracy_benchmark()

        print("  ðŸ’¾ Measuring memory usage...")
        self.results['memory'] = self.benchmark.run_memory_benchmark()

        # Simulate energy benchmark (would require specialized hardware)
        print("  âš¡ Estimating energy efficiency...")
        self.results['energy'] = self._estimate_energy_efficiency()

        return self.results

    def _estimate_energy_efficiency(self) -> Dict[str, BenchmarkResult]:
        """Estimate energy efficiency (simplified simulation)."""
        energy_results = {}

        for i, model in enumerate(self.models):
            model_name = getattr(model, 'name', f'model_{i}')

            # Energy roughly correlates with latency * memory usage
            if 'latency' in self.results and 'memory' in self.results:
                latency_result = self.results['latency'].get(model_name)
                memory_result = self.results['memory'].get(model_name)

                if latency_result and memory_result:
                    # Energy âˆ power Ã— time, power âˆ memory usage
                    energy_values = []
                    for lat, mem in zip(latency_result.values, memory_result.values):
                        # Simplified energy model: energy = base + latency_factor * time + memory_factor * memory
                        energy = 0.1 + (lat / 1000) * 2.0 + mem * 0.01  # Joules
                        energy_values.append(energy)

                    energy_results[model_name] = BenchmarkResult(
                        f"{model_name}_energy_joules",
                        energy_values,
                        metadata={'estimated': True, **self.benchmark.system_info}
                    )

        # Fallback if no latency/memory results
        if not energy_results:
            for i, model in enumerate(self.models):
                model_name = getattr(model, 'name', f'model_{i}')
                # Simulate energy measurements
                energy_values = [0.5 + np.random.normal(0, 0.1) for _ in range(5)]
                energy_results[model_name] = BenchmarkResult(
                    f"{model_name}_energy_joules",
                    energy_values,
                    metadata={'estimated': True, **self.benchmark.system_info}
                )

        return energy_results

    def plot_results(self, save_plots: bool = True):
        """Generate visualization plots for benchmark results."""
        if not self.results:
            print("No results to plot. Run benchmark first.")
            return
        
        if not MATPLOTLIB_AVAILABLE:
            print("âš ï¸ matplotlib not available - skipping plots. Install with: pip install matplotlib")
            return

        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('ML Model Benchmark Results', fontsize=16, fontweight='bold')

        # Plot each metric type
        metrics = ['latency', 'accuracy', 'memory', 'energy']
        units = ['ms', 'accuracy', 'MB', 'J']

        for idx, (metric, unit) in enumerate(zip(metrics, units)):
            ax = axes[idx // 2, idx % 2]

            if metric in self.results:
                model_names = []
                means = []
                stds = []

                for model_name, result in self.results[metric].items():
                    clean_name = model_name.replace(f'_{metric}', '').replace('_ms', '').replace('_mb', '').replace('_joules', '')
                    model_names.append(clean_name)
                    means.append(result.mean)
                    stds.append(result.std)

                bars = ax.bar(model_names, means, yerr=stds, capsize=5, alpha=0.7)
                ax.set_title(f'{metric.capitalize()} Comparison')
                ax.set_ylabel(f'{metric.capitalize()} ({unit})')
                ax.tick_params(axis='x', rotation=45)

                # Color bars by performance (green = better)
                if metric in ['latency', 'memory', 'energy']:  # Lower is better
                    best_idx = means.index(min(means))
                else:  # Higher is better (accuracy)
                    best_idx = means.index(max(means))

                for i, bar in enumerate(bars):
                    if i == best_idx:
                        bar.set_color('green')
                        bar.set_alpha(0.8)
            else:
                ax.text(0.5, 0.5, f'No {metric} data', ha='center', va='center', transform=ax.transAxes)
                ax.set_title(f'{metric.capitalize()} Comparison')

        plt.tight_layout()

        if save_plots:
            plot_path = self.output_dir / 'benchmark_comparison.png'
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            print(f"ðŸ“Š Plots saved to {plot_path}")

        plt.show()

    def plot_pareto_frontier(self, x_metric: str = 'latency', y_metric: str = 'accuracy'):
        """Plot Pareto frontier for two competing objectives."""
        if not MATPLOTLIB_AVAILABLE:
            print("âš ï¸ matplotlib not available - skipping plots. Install with: pip install matplotlib")
            return
            
        if x_metric not in self.results or y_metric not in self.results:
            print(f"Missing data for {x_metric} or {y_metric}")
            return

        plt.figure(figsize=(10, 8))

        x_values = []
        y_values = []
        model_names = []

        for model_name in self.results[x_metric].keys():
            clean_name = model_name.replace(f'_{x_metric}', '').replace('_ms', '').replace('_mb', '').replace('_joules', '')
            if clean_name in [mn.replace(f'_{y_metric}', '') for mn in self.results[y_metric].keys()]:
                x_val = self.results[x_metric][model_name].mean

                # Find corresponding y value
                y_key = None
                for key in self.results[y_metric].keys():
                    if clean_name in key:
                        y_key = key
                        break

                if y_key:
                    y_val = self.results[y_metric][y_key].mean
                    x_values.append(x_val)
                    y_values.append(y_val)
                    model_names.append(clean_name)

        # Plot points
        plt.scatter(x_values, y_values, s=100, alpha=0.7)

        # Label points
        for i, name in enumerate(model_names):
            plt.annotate(name, (x_values[i], y_values[i]),
                        xytext=(5, 5), textcoords='offset points')

        # Determine if lower or higher is better for each metric
        x_lower_better = x_metric in ['latency', 'memory', 'energy']
        y_lower_better = y_metric in ['latency', 'memory', 'energy']

        plt.xlabel(f'{x_metric.capitalize()} ({"lower" if x_lower_better else "higher"} is better)')
        plt.ylabel(f'{y_metric.capitalize()} ({"lower" if y_lower_better else "higher"} is better)')
        plt.title(f'Pareto Frontier: {x_metric.capitalize()} vs {y_metric.capitalize()}')
        plt.grid(True, alpha=0.3)

        # Save plot
        plot_path = self.output_dir / f'pareto_{x_metric}_vs_{y_metric}.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        print(f"ðŸ“Š Pareto plot saved to {plot_path}")
        plt.show()

    def generate_report(self) -> str:
        """Generate comprehensive benchmark report."""
        if not self.results:
            return "No benchmark results available. Run benchmark first."

        report_lines = []
        report_lines.append("# ML Model Benchmark Report")
        report_lines.append("=" * 50)
        report_lines.append("")

        # System information
        report_lines.append("## System Information")
        system_info = self.benchmark.system_info
        for key, value in system_info.items():
            report_lines.append(f"- {key}: {value}")
        report_lines.append("")

        # Results summary
        report_lines.append("## Benchmark Results Summary")
        report_lines.append("")

        for metric_type, results in self.results.items():
            report_lines.append(f"### {metric_type.capitalize()} Results")
            report_lines.append("")

            # Find best performer
            if metric_type in ['latency', 'memory', 'energy']:
                # Lower is better
                best_model = min(results.items(), key=lambda x: x[1].mean)
                comparison_text = "fastest" if metric_type == 'latency' else "most efficient"
            else:
                # Higher is better
                best_model = max(results.items(), key=lambda x: x[1].mean)
                comparison_text = "most accurate"

            report_lines.append(f"**Best performer**: {best_model[0]} ({comparison_text})")
            report_lines.append("")

            # Detailed results
            for model_name, result in results.items():
                clean_name = model_name.replace(f'_{metric_type}', '').replace('_ms', '').replace('_mb', '').replace('_joules', '')
                report_lines.append(f"- **{clean_name}**: {result.mean:.4f} Â± {result.std:.4f}")
            report_lines.append("")

        # Recommendations
        report_lines.append("## Recommendations")
        report_lines.append("")

        if len(self.results) >= 2:
            # Find overall best trade-off model
            if 'latency' in self.results and 'accuracy' in self.results:
                report_lines.append("### Accuracy vs Speed Trade-off")

                # Simple scoring: normalize metrics and combine
                latency_results = self.results['latency']
                accuracy_results = self.results['accuracy']

                scores = {}
                for model_name in latency_results.keys():
                    clean_name = model_name.replace('_latency', '').replace('_ms', '')

                    # Find corresponding accuracy
                    acc_key = None
                    for key in accuracy_results.keys():
                        if clean_name in key:
                            acc_key = key
                            break

                    if acc_key:
                        # Normalize: latency (lower better), accuracy (higher better)
                        lat_vals = [r.mean for r in latency_results.values()]
                        acc_vals = [r.mean for r in accuracy_results.values()]

                        norm_latency = 1 - (latency_results[model_name].mean - min(lat_vals)) / (max(lat_vals) - min(lat_vals) + 1e-8)
                        norm_accuracy = (accuracy_results[acc_key].mean - min(acc_vals)) / (max(acc_vals) - min(acc_vals) + 1e-8)

                        # Combined score (equal weight)
                        scores[clean_name] = (norm_latency + norm_accuracy) / 2

                if scores:
                    best_overall = max(scores.items(), key=lambda x: x[1])
                    report_lines.append(f"- **Best overall trade-off**: {best_overall[0]} (score: {best_overall[1]:.3f})")
                    report_lines.append("")

        report_lines.append("### Usage Recommendations")
        if 'accuracy' in self.results and 'latency' in self.results:
            acc_results = self.results['accuracy']
            lat_results = self.results['latency']

            # Find highest accuracy model
            best_acc_model = max(acc_results.items(), key=lambda x: x[1].mean)
            best_lat_model = min(lat_results.items(), key=lambda x: x[1].mean)

            report_lines.append(f"- **For maximum accuracy**: Use {best_acc_model[0].replace('_accuracy', '')}")
            report_lines.append(f"- **For minimum latency**: Use {best_lat_model[0].replace('_latency_ms', '')}")
            report_lines.append("- **For production deployment**: Consider the best overall trade-off model above")

        report_lines.append("")
        report_lines.append("---")
        report_lines.append("Report generated by TinyTorch Benchmarking Suite")

        # Save report
        report_text = "\n".join(report_lines)
        report_path = self.output_dir / 'benchmark_report.md'
        with open(report_path, 'w') as f:
            f.write(report_text)

        print(f"ðŸ“„ Report saved to {report_path}")
        return report_text
    ### END SOLUTION

def test_unit_benchmark_suite():
    """ðŸ”¬ Test BenchmarkSuite comprehensive functionality."""
    print("ðŸ”¬ Unit Test: BenchmarkSuite...")

    # Create mock models
    class MockModel:
        def __init__(self, name):
            self.name = name

        def forward(self, x):
            time.sleep(0.001)
            return x

    models = [MockModel("efficient_model"), MockModel("accurate_model")]
    datasets = [{"test": "data"}]

    # Create temporary directory for test output
    import tempfile
    with tempfile.TemporaryDirectory() as tmp_dir:
        suite = BenchmarkSuite(models, datasets, output_dir=tmp_dir)

        # Run full benchmark
        results = suite.run_full_benchmark()

        # Verify all benchmark types completed
        assert 'latency' in results
        assert 'accuracy' in results
        assert 'memory' in results
        assert 'energy' in results

        # Verify results structure
        for metric_results in results.values():
            assert len(metric_results) == 2  # Two models
            assert all(isinstance(result, BenchmarkResult) for result in metric_results.values())

        # Test report generation
        report = suite.generate_report()
        assert "Benchmark Report" in report
        assert "System Information" in report
        assert "Recommendations" in report

        # Verify files are created
        output_path = Path(tmp_dir)
        assert (output_path / 'benchmark_report.md').exists()

    print("âœ… BenchmarkSuite works correctly!")

test_unit_benchmark_suite()
